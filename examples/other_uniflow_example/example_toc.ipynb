{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to write a table of content (TOC) for all example notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -q uniflow==0.0.24\n",
    "\n",
    "import pandas as pd\n",
    "# Adjusting display settings to avoid truncation\n",
    "pd.set_option('display.max_rows', None)  # Adjust to display all rows\n",
    "pd.set_option('display.max_columns', None)  # Adjust to display all columns\n",
    "pd.set_option('display.width', None)  # Adjust to ensure each row uses optimal width\n",
    "pd.set_option('display.max_colwidth', None)  # Adjust to display full content of each cell\n",
    "\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.flow_factory import FlowFactory\n",
    "from uniflow.flow.config import TransformConfig\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.viz import Viz\n",
    "from uniflow.op.prompt import PromptTemplate, Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_ipynb_files(directory):\n",
    "    ipynb_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.ipynb'):\n",
    "                ipynb_files.append(os.path.join(root, file))\n",
    "    return ipynb_files\n",
    "\n",
    "# Replace with the actual path to the cloned 'example' directory\n",
    "all_ipynb = list_ipynb_files('./')\n",
    "len(all_ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./example_toc.ipynb',\n",
       " './rater/huggingface_classification.ipynb',\n",
       " './rater/bedrock_classification.ipynb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ipynb[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_html = \"https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/example_toc.ipynb',\n",
       " 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/huggingface_classification.ipynb',\n",
       " 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/bedrock_classification.ipynb']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ipynb_htmls = [home_html+ipynb[1:] for ipynb in all_ipynb] ## remove the first file, which is this notebook\n",
    "all_ipynb_htmls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(context={'filename': 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/example_toc.ipynb'}),\n",
       " Context(context={'filename': 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/huggingface_classification.ipynb'}),\n",
       " Context(context={'filename': 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/bedrock_classification.ipynb'})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ipynb_htmls_context = [Context(context={\"filename\": c}) for c in all_ipynb_htmls]\n",
    "all_ipynb_htmls_context[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_config = TransformConfig(\n",
    "    flow_name=\"TransformOpenAIFlow\",\n",
    "    model_config=OpenAIModelConfig(\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0),\n",
    "    prompt_template=PromptTemplate(\n",
    "            instruction=\"\"\"\n",
    "            Assume you are an experienced ML technical writer. Here is a list of jupyter notebooks. First, identify the formal title of each notebook, i.e. the notebook header. Only one title per each notebook. Then, provide a 3-sentence, concise and informative summary of each notebook. Next, identify the input data type each notebook is using (including PDF, HTML, TXT, Jupyter Notebook, etc). Last, identify the uniflow model type each notebook is using (such as 'TransformAzureOpenAIFlow', 'TransformGoogleFlow', 'TransformGoogleMultiModalModelFlow','TransformHuggingFaceFlow', 'TransformLMQGFlow', 'TransformOpenAIFlow', etc.)\n",
    "            Follow the format of the examples below to include each notebook's title, summary, input_data_type, uniflow_type and url in response. \n",
    "            \"\"\",\n",
    "            few_shot_prompt=[\n",
    "                Context(\n",
    "                    context=\"\"\"...\"\"\",\n",
    "                    title=\"\"\"...\"\"\",\n",
    "                    summary=\"\"\"...\"\"\",\n",
    "                    input_data_type=\"\"\"...\"\"\",\n",
    "                    uniflow_type=\"\"\"...\"\"\",\n",
    "                    url=\"\"\"...\"\"\",\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'output': [{'response': [{'notebooks': [{'title': 'Huggingface Classification',\n",
       "        'summary': 'This notebook demonstrates how to use Huggingface models for text classification tasks. It includes fine-tuning a pre-trained model on a custom dataset and evaluating its performance.',\n",
       "        'input_data_type': 'Jupyter Notebook',\n",
       "        'uniflow_type': 'TransformHuggingFaceFlow',\n",
       "        'url': 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/huggingface_classification.ipynb'}]}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x124850d90>},\n",
       " {'output': [{'response': [{'few_shot_prompt': [{'context': 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/bedrock_classification.ipynb',\n",
       "        'title': 'Bedrock Classification',\n",
       "        'summary': 'This notebook demonstrates the process of classifying different types of bedrock using machine learning models. It includes data preprocessing, model training, and evaluation of the classification performance.',\n",
       "        'input_data_type': 'Jupyter Notebook',\n",
       "        'uniflow_type': 'TransformLMQGFlow',\n",
       "        'url': 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/bedrock_classification.ipynb'}]}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x1248f9e70>},\n",
       " {'output': [{'response': [{'notebooks': [{'title': 'LLM-based PDF Extraction and Text Cleaning',\n",
       "        'summary': 'This notebook demonstrates the process of extracting text from PDF documents using LLM-based models and cleaning the extracted text for further analysis. It includes steps for preprocessing the text data and preparing it for clustering or classification tasks.',\n",
       "        'input_data_type': 'PDF',\n",
       "        'uniflow_type': 'TransformHuggingFaceFlow',\n",
       "        'url': 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/classification.ipynb'},\n",
       "       {'title': 'Data Clustering with LLM-based Models',\n",
       "        'summary': 'This notebook focuses on using LLM-based models for clustering text data. It covers the process of transforming the text data using LLM-based embeddings and applying clustering algorithms to group similar documents together.',\n",
       "        'input_data_type': 'Jupyter Notebook',\n",
       "        'uniflow_type': 'TransformHuggingFaceFlow',\n",
       "        'url': 'https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering/tree/main/example/rater/classification.ipynb'}]}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x124850250>},\n",
       " {'output': [{'response': [{'notebooks': [{'title': 'Text Classification with BERT',\n",
       "        'summary': 'This notebook demonstrates how to perform text classification using BERT pre-trained model. It includes data preprocessing, model training, and evaluation.',\n",
       "        'input_data_type': 'Jupyter Notebook',\n",
       "        'uniflow_type': 'TransformHuggingFaceFlow',\n",
       "        'url': 'https://github.com/user/text-classification-bert.ipynb'},\n",
       "       {'title': 'Named Entity Recognition with SpaCy',\n",
       "        'summary': 'This notebook showcases the process of named entity recognition using SpaCy library. It covers data annotation, model training, and entity extraction.',\n",
       "        'input_data_type': 'Jupyter Notebook',\n",
       "        'uniflow_type': 'TransformLMQGFlow',\n",
       "        'url': 'https://github.com/user/named-entity-recognition-spacy.ipynb'},\n",
       "       {'title': 'Data Visualization with Plotly',\n",
       "        'summary': 'This notebook provides a comprehensive guide to creating interactive data visualizations using Plotly library. It includes examples of various plot types and customization options.',\n",
       "        'input_data_type': 'Jupyter Notebook',\n",
       "        'uniflow_type': 'TransformOpenAIFlow',\n",
       "        'url': 'https://github.com/user/data-visualization-plotly.ipynb'}]}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x124c0f280>}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = TransformClient(transform_config)\n",
    "client_output = client.run(all_ipynb_htmls_context[1:5])\n",
    "client_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>input_data_type</th>\n",
       "      <th>uniflow_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huggingface_classification.ipynb</td>\n",
       "      <td>This notebook demonstrates the use of Hugging Face models for text classification tasks. It includes loading a pre-trained model, fine-tuning on a custom dataset, and evaluating the model's performance.</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>TransformHuggingFaceFlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bedrock_classification.ipynb</td>\n",
       "      <td>This notebook demonstrates the process of classifying bedrock types using machine learning. It includes data preprocessing, model training, and evaluation.</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>TransformHuggingFaceFlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM-based PDF Extraction and Text Cleaning</td>\n",
       "      <td>This notebook demonstrates the process of using Large Language Models (LLM) for extracting text from PDF documents and cleaning the extracted text. It covers the steps of preprocessing the PDF, extracting text using LLM, and performing text cleaning to improve the quality of the extracted text.</td>\n",
       "      <td>PDF</td>\n",
       "      <td>TransformHuggingFaceFlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Clustering with LLM-based Embeddings</td>\n",
       "      <td>This notebook showcases the application of Large Language Models (LLM) for generating embeddings of text data and using these embeddings for data clustering. It includes the steps of preprocessing the text data, generating LLM-based embeddings, and performing clustering using the embeddings.</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>TransformHuggingFaceFlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDF Extraction and Text Cleaning</td>\n",
       "      <td>This notebook demonstrates the process of extracting text from PDF documents and cleaning the text data for further analysis. It includes techniques for handling special characters, removing stop words, and performing lemmatization.</td>\n",
       "      <td>PDF</td>\n",
       "      <td>TransformHuggingFaceFlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Clustering with LLM-based Model</td>\n",
       "      <td>This notebook showcases the use of a large language model (LLM) for data clustering tasks. It covers the process of fine-tuning the LLM for clustering, applying it to the input data, and visualizing the clustered results.</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>TransformOpenAIFlow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title  \\\n",
       "0            huggingface_classification.ipynb   \n",
       "1                bedrock_classification.ipynb   \n",
       "2  LLM-based PDF Extraction and Text Cleaning   \n",
       "3   Data Clustering with LLM-based Embeddings   \n",
       "4            PDF Extraction and Text Cleaning   \n",
       "5        Data Clustering with LLM-based Model   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   summary  \\\n",
       "0                                                                                               This notebook demonstrates the use of Hugging Face models for text classification tasks. It includes loading a pre-trained model, fine-tuning on a custom dataset, and evaluating the model's performance.   \n",
       "1                                                                                                                                              This notebook demonstrates the process of classifying bedrock types using machine learning. It includes data preprocessing, model training, and evaluation.   \n",
       "2  This notebook demonstrates the process of using Large Language Models (LLM) for extracting text from PDF documents and cleaning the extracted text. It covers the steps of preprocessing the PDF, extracting text using LLM, and performing text cleaning to improve the quality of the extracted text.   \n",
       "3     This notebook showcases the application of Large Language Models (LLM) for generating embeddings of text data and using these embeddings for data clustering. It includes the steps of preprocessing the text data, generating LLM-based embeddings, and performing clustering using the embeddings.   \n",
       "4                                                                 This notebook demonstrates the process of extracting text from PDF documents and cleaning the text data for further analysis. It includes techniques for handling special characters, removing stop words, and performing lemmatization.   \n",
       "5                                                                            This notebook showcases the use of a large language model (LLM) for data clustering tasks. It covers the process of fine-tuning the LLM for clustering, applying it to the input data, and visualizing the clustered results.   \n",
       "\n",
       "    input_data_type              uniflow_type  \n",
       "0  Jupyter Notebook  TransformHuggingFaceFlow  \n",
       "1  Jupyter Notebook  TransformHuggingFaceFlow  \n",
       "2               PDF  TransformHuggingFaceFlow  \n",
       "3  Jupyter Notebook  TransformHuggingFaceFlow  \n",
       "4               PDF  TransformHuggingFaceFlow  \n",
       "5  Jupyter Notebook       TransformOpenAIFlow  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten the data\n",
    "flattened_data = []\n",
    "for item in client_output:\n",
    "    for output in item['output']:\n",
    "        for response in output['response']:\n",
    "            for notebook in response['notebooks']:\n",
    "                flattened_data.append({\n",
    "                    'title': notebook['title'],\n",
    "                    'summary': notebook['summary'],\n",
    "                    'input_data_type': notebook['input_data_type'],\n",
    "                    'uniflow_type': notebook['uniflow_type']\n",
    "                })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4's output is unstable and the below parsing doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m name, summary, input_data_type, uniflow_type, url \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o, u \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(output, all_ipynb_htmls_context[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m5\u001b[39m]):\n\u001b[0;32m----> 4\u001b[0m     name\u001b[38;5;241m.\u001b[39mappend(\u001b[43mo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m     summary\u001b[38;5;241m.\u001b[39mappend(o[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m     input_data_type\u001b[38;5;241m.\u001b[39mappend(o[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_data_type\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "title, summary, input_data_type, uniflow_type, url = [], [], [], [], []\n",
    "\n",
    "for o, u in zip(output, all_ipynb_htmls_context[1:5]):\n",
    "    title.append(o[0]['output'][0]['response'][0]['title'])\n",
    "    summary.append(o[0]['output'][0]['response'][0]['summary'])\n",
    "    input_data_type.append(o[0]['output'][0]['response'][0]['input_data_type'])\n",
    "    uniflow_type.append(o[0]['output'][0]['response'][0]['uniflow_type'])\n",
    "    url.append(u)\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'title': title,\n",
    "    'summary': summary,\n",
    "    'input_data_type': input_data_type,\n",
    "    'uniflow_type': uniflow_type,\n",
    "    'URL': url\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df['URL'] = df['URL'].apply(lambda x: f'<a href=\"{x}\" target=\"_blank\">{x}</a>')\n",
    "\n",
    "\n",
    "# Displaying the DataFrame\n",
    "display(HTML(df.to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
