<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>22.11. Information Theory &#8212; Dive into Deep Learning 1.0.3 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="23. Appendix: Tools for Deep Learning" href="../chapter_appendix-tools-for-deep-learning/index.html" />
    <link rel="prev" title="22.10. Statistics" href="statistics.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">22. </span>Appendix: Mathematics for Deep Learning</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">22.11. </span>Information Theory</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_appendix-mathematics-for-deep-learning/information-theory.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="http://preview.d2l.ai/d2l-en/master">
                  <i class="fas fa-book"></i>
                  Preview Version
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en-mxnet.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MXNet
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.zip">
                  <i class="fab fa-python"></i>
                  Notebooks
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai">
                  <i class="fas fa-user-graduate"></i>
                  Courses
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh.d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/lazy-init.html">6.4. Lazy Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.5. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.6. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.7. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">8.2. Networks Using Blocks (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">8.3. Network in Network (NiN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.4. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.5. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.7. Densely Connected Networks (DenseNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/cnn-design.html">8.8. Designing Convolution Network Architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">10.4. Bidirectional Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.5. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.6. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.7. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.8. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/index.html">11. Attention Mechanisms and Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">13. Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize.html">13.1. Compilers and Interpreters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation.html">13.2. Asynchronous Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism.html">13.3. Automatic Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware.html">13.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus.html">13.5. Training on Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise.html">13.6. Concise Implementation for Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver.html">13.7. Parameter Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">14. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">14.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">14.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">14.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">14.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">14.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">14.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">14.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">14.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">14.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">14.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">14.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">14.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">14.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">15. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">15.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">15.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">15.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">15.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">15.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">15.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">15.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">15.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">15.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">15.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">16. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">16.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">16.2. Sentiment Analysis: Using Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">16.3. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">16.4. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">16.5. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">16.7. Natural Language Inference: Fine-Tuning BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">17. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">17.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">17.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">17.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">18. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">18.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">18.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">18.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">19. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">19.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">19.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">19.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">19.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">19.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index.html">20. Generative Adversarial Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan.html">20.1. Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan.html">20.2. Deep Convolutional Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index.html">21. Recommender Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro.html">21.1. Overview of Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens.html">21.2. The MovieLens Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf.html">21.3. Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec.html">21.4. AutoRec: Rating Prediction with Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking.html">21.5. Personalized Ranking for Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf.html">21.6. Neural Collaborative Filtering for Personalized Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec.html">21.7. Sequence-Aware Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr.html">21.8. Feature-Rich Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm.html">21.9. Factorization Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm.html">21.10. Deep Factorization Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">22. Appendix: Mathematics for Deep Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops.html">22.1. Geometry and Linear Algebraic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition.html">22.2. Eigendecompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="single-variable-calculus.html">22.3. Single Variable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="multivariable-calculus.html">22.4. Multivariable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="integral-calculus.html">22.5. Integral Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-variables.html">22.6. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood.html">22.7. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html">22.8. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes.html">22.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics.html">22.10. Statistics</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">22.11. Information Theory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">23. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">23.1. Using Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">23.2. Using Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">23.3. Using AWS EC2 Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab.html">23.4. Using Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">23.5. Selecting Servers and GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">23.6. Contributing to This Book</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">23.7. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">23.8. The <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API Document</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">References</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/lazy-init.html">6.4. Lazy Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.5. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.6. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.7. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">8.2. Networks Using Blocks (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">8.3. Network in Network (NiN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.4. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.5. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.7. Densely Connected Networks (DenseNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/cnn-design.html">8.8. Designing Convolution Network Architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">10.4. Bidirectional Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.5. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.6. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.7. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.8. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/index.html">11. Attention Mechanisms and Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">13. Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize.html">13.1. Compilers and Interpreters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation.html">13.2. Asynchronous Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism.html">13.3. Automatic Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware.html">13.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus.html">13.5. Training on Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise.html">13.6. Concise Implementation for Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver.html">13.7. Parameter Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">14. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">14.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">14.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">14.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">14.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">14.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">14.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">14.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">14.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">14.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">14.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">14.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">14.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">14.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">15. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">15.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">15.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">15.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">15.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">15.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">15.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">15.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">15.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">15.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">15.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">16. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">16.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">16.2. Sentiment Analysis: Using Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">16.3. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">16.4. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">16.5. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">16.7. Natural Language Inference: Fine-Tuning BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">17. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">17.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">17.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">17.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">18. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">18.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">18.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">18.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">19. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">19.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">19.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">19.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">19.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">19.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index.html">20. Generative Adversarial Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan.html">20.1. Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan.html">20.2. Deep Convolutional Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index.html">21. Recommender Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro.html">21.1. Overview of Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens.html">21.2. The MovieLens Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf.html">21.3. Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec.html">21.4. AutoRec: Rating Prediction with Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking.html">21.5. Personalized Ranking for Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf.html">21.6. Neural Collaborative Filtering for Personalized Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec.html">21.7. Sequence-Aware Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr.html">21.8. Feature-Rich Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm.html">21.9. Factorization Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm.html">21.10. Deep Factorization Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">22. Appendix: Mathematics for Deep Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops.html">22.1. Geometry and Linear Algebraic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition.html">22.2. Eigendecompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="single-variable-calculus.html">22.3. Single Variable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="multivariable-calculus.html">22.4. Multivariable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="integral-calculus.html">22.5. Integral Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-variables.html">22.6. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood.html">22.7. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html">22.8. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes.html">22.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics.html">22.10. Statistics</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">22.11. Information Theory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">23. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">23.1. Using Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">23.2. Using Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">23.3. Using AWS EC2 Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab.html">23.4. Using Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">23.5. Selecting Servers and GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">23.6. Contributing to This Book</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">23.7. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">23.8. The <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API Document</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">References</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="information-theory">
<span id="sec-information-theory"></span><h1><span class="section-number">22.11. </span>Information Theory<a class="headerlink" href="#information-theory" title="Permalink to this heading">¶</a><div class="d2l-tabs" style="float:right"><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb'); return false;"> <button style="float:right", id="Colab_[pytorch]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [pytorch] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[pytorch]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb'); return false;"> <button style="float:right", id="Colab_[mxnet]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [mxnet] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[mxnet]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb'); return false;"> <button style="float:right", id="Colab_[jax]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [jax] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[jax]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb'); return false;"> <button style="float:right", id="Colab_[tensorflow]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [tensorflow] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[tensorflow]"> Open the notebook in Colab</div></div></div><a href="https://studiolab.sagemaker.aws/import/github/d2l-ai/d2l-pytorch-sagemaker-studio-lab/blob/main/GettingStarted-D2L.ipynb" onclick="captureOutboundLink('https://studiolab.sagemaker.aws/import/github/d2l-ai/d2l-pytorch-sagemaker-studio-lab/blob/main/GettingStarted-D2L.ipynb'); return false;"> <button style="float:right", id="SageMaker_Studio_Lab" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> SageMaker Studio Lab </button></a><div class="mdl-tooltip" data-mdl-for="SageMaker_Studio_Lab"> Open the notebook in SageMaker Studio Lab</div></h1>
<p>The universe is overflowing with information. Information provides a
common language across disciplinary rifts: from Shakespeare’s Sonnet to
researchers’ paper on Cornell ArXiv, from Van Gogh’s printing Starry
Night to Beethoven’s music Symphony No. 5, from the first programming
language Plankalkül to the state-of-the-art machine learning algorithms.
Everything must follow the rules of information theory, no matter the
format. With information theory, we can measure and compare how much
information is present in different signals. In this section, we will
investigate the fundamental concepts of information theory and
applications of information theory in machine learning.</p>
<p>Before we get started, let’s outline the relationship between machine
learning and information theory. Machine learning aims to extract
interesting signals from data and make critical predictions. On the
other hand, information theory studies encoding, decoding, transmitting,
and manipulating information. As a result, information theory provides
fundamental language for discussing the information processing in
machine learned systems. For example, many machine learning applications
use the cross-entropy loss as described in <a class="reference internal" href="../chapter_linear-classification/softmax-regression.html#sec-softmax"><span class="std std-numref">Section 4.1</span></a>. This
loss can be directly derived from information theoretic considerations.</p>
<div class="section" id="information">
<h2><span class="section-number">22.11.1. </span>Information<a class="headerlink" href="#information" title="Permalink to this heading">¶</a></h2>
<p>Let’s start with the “soul” of information theory: information.
<em>Information</em> can be encoded in anything with a particular sequence of
one or more encoding formats. Suppose that we task ourselves with trying
to define a notion of information. What could be our starting point?</p>
<p>Consider the following thought experiment. We have a friend with a deck
of cards. They will shuffle the deck, flip over some cards, and tell us
statements about the cards. We will try to assess the information
content of each statement.</p>
<p>First, they flip over a card and tell us, “I see a card.” This provides
us with no information at all. We were already certain that this was the
case so we hope the information should be zero.</p>
<p>Next, they flip over a card and say, “I see a heart.” This provides us
some information, but in reality there are only <span class="math notranslate nohighlight">\(4\)</span> different
suits that were possible, each equally likely, so we are not surprised
by this outcome. We hope that whatever the measure of information, this
event should have low information content.</p>
<p>Next, they flip over a card and say, “This is the <span class="math notranslate nohighlight">\(3\)</span> of spades.”
This is more information. Indeed there were <span class="math notranslate nohighlight">\(52\)</span> equally likely
possible outcomes, and our friend told us which one it was. This should
be a medium amount of information.</p>
<p>Let’s take this to the logical extreme. Suppose that finally they flip
over every card from the deck and read off the entire sequence of the
shuffled deck. There are <span class="math notranslate nohighlight">\(52!\)</span> different orders to the deck, again
all equally likely, so we need a lot of information to know which one it
is.</p>
<p>Any notion of information we develop must conform to this intuition.
Indeed, in the next sections we will learn how to compute that these
events have <span class="math notranslate nohighlight">\(0\textrm{ bits}\)</span>, <span class="math notranslate nohighlight">\(2\textrm{ bits}\)</span>,
<span class="math notranslate nohighlight">\(~5.7\textrm{ bits}\)</span>, and <span class="math notranslate nohighlight">\(~225.6\textrm{ bits}\)</span> of
information respectively.</p>
<p>If we read through these thought experiments, we see a natural idea. As
a starting point, rather than caring about the knowledge, we may build
off the idea that information represents the degree of surprise or the
abstract possibility of the event. For example, if we want to describe
an unusual event, we need a lot information. For a common event, we may
not need much information.</p>
<p>In 1948, Claude E. Shannon published <em>A Mathematical Theory of
Communication</em> <span id="id1">(<a class="reference internal" href="../chapter_references/zreferences.html#id256" title="Shannon, C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal, 27(3), 379–423.">Shannon, 1948</a>)</span> establishing the theory of
information. In his article, Shannon introduced the concept of
information entropy for the first time. We will begin our journey here.</p>
<div class="section" id="self-information">
<h3><span class="section-number">22.11.1.1. </span>Self-information<a class="headerlink" href="#self-information" title="Permalink to this heading">¶</a></h3>
<p>Since information embodies the abstract possibility of an event, how do
we map the possibility to the number of bits? Shannon introduced the
terminology <em>bit</em> as the unit of information, which was originally
created by John Tukey. So what is a “bit” and why do we use it to
measure information? Historically, an antique transmitter can only send
or receive two types of code: <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Indeed, binary
encoding is still in common use on all modern digital computers. In this
way, any information is encoded by a series of <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>.
And hence, a series of binary digits of length <span class="math notranslate nohighlight">\(n\)</span> contains
<span class="math notranslate nohighlight">\(n\)</span> bits of information.</p>
<p>Now, suppose that for any series of codes, each <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>
occurs with a probability of <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span>. Hence, an event
<span class="math notranslate nohighlight">\(X\)</span> with a series of codes of length <span class="math notranslate nohighlight">\(n\)</span>, occurs with a
probability of <span class="math notranslate nohighlight">\(\frac{1}{2^n}\)</span>. At the same time, as we mentioned
before, this series contains <span class="math notranslate nohighlight">\(n\)</span> bits of information. So, can we
generalize to a mathematical function which can transfer the probability
<span class="math notranslate nohighlight">\(p\)</span> to the number of bits? Shannon gave the answer by defining
<em>self-information</em></p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-0">
<span class="eqno">(22.11.1)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-0" title="Permalink to this equation">¶</a></span>\[I(X) = - \log_2 (p),\]</div>
<p>as the <em>bits</em> of information we have received for this event <span class="math notranslate nohighlight">\(X\)</span>.
Note that we will always use base-2 logarithms in this section. For the
sake of simplicity, the rest of this section will omit the subscript 2
in the logarithm notation, i.e., <span class="math notranslate nohighlight">\(\log(.)\)</span> always refers to
<span class="math notranslate nohighlight">\(\log_2(.)\)</span>. For example, the code “0010” has a self-information</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-1">
<span class="eqno">(22.11.2)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-1" title="Permalink to this equation">¶</a></span>\[I(\textrm{&quot;0010&quot;}) = - \log (p(\textrm{&quot;0010&quot;})) = - \log \left( \frac{1}{2^4} \right) = 4 \textrm{ bits}.\]</div>
<p>We can calculate self information as shown below. Before that, let’s
first import all the necessary packages in this section.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-1-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-1-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-1-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-1-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">NLLLoss</span>


<span class="k">def</span> <span class="nf">nansum</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Define nansum, as pytorch does not offer it inbuilt.</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">self_information</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">p</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="n">self_information</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">6.0</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-1-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">mxnet.metric</span> <span class="kn">import</span> <span class="n">NegativeLogLikelihood</span>
<span class="kn">from</span> <span class="nn">mxnet.ndarray</span> <span class="kn">import</span> <span class="n">nansum</span>


<span class="k">def</span> <span class="nf">self_information</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="n">self_information</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">6.0</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-1-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>


<span class="k">def</span> <span class="nf">log2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">nansum</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span>
        <span class="n">x</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">self_information</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">log2</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">p</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">self_information</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">6.0</span>
</pre></div>
</div>
</div></div></div>
</div>
<div class="section" id="entropy">
<h2><span class="section-number">22.11.2. </span>Entropy<a class="headerlink" href="#entropy" title="Permalink to this heading">¶</a></h2>
<p>As self-information only measures the information of a single discrete
event, we need a more generalized measure for any random variable of
either discrete or continuous distribution.</p>
<div class="section" id="motivating-entropy">
<h3><span class="section-number">22.11.2.1. </span>Motivating Entropy<a class="headerlink" href="#motivating-entropy" title="Permalink to this heading">¶</a></h3>
<p>Let’s try to get specific about what we want. This will be an informal
statement of what are known as the <em>axioms of Shannon entropy</em>. It will
turn out that the following collection of common-sense statements force
us to a unique definition of information. A formal version of these
axioms, along with several others may be found in
<span id="id2">Csiszár (<a class="reference internal" href="../chapter_references/zreferences.html#id48" title="Csiszár, I. (2008). Axiomatic characterizations of information measures. Entropy, 10(3), 261–273.">2008</a>)</span>.</p>
<ol class="arabic simple">
<li><p>The information we gain by observing a random variable does not
depend on what we call the elements, or the presence of additional
elements which have probability zero.</p></li>
<li><p>The information we gain by observing two random variables is no more
than the sum of the information we gain by observing them separately.
If they are independent, then it is exactly the sum.</p></li>
<li><p>The information gained when observing (nearly) certain events is
(nearly) zero.</p></li>
</ol>
<p>While proving this fact is beyond the scope of our text, it is important
to know that this uniquely determines the form that entropy must take.
The only ambiguity that these allow is in the choice of fundamental
units, which is most often normalized by making the choice we saw before
that the information provided by a single fair coin flip is one bit.</p>
</div>
<div class="section" id="definition">
<h3><span class="section-number">22.11.2.2. </span>Definition<a class="headerlink" href="#definition" title="Permalink to this heading">¶</a></h3>
<p>For any random variable <span class="math notranslate nohighlight">\(X\)</span> that follows a probability
distribution <span class="math notranslate nohighlight">\(P\)</span> with a probability density function (p.d.f.) or a
probability mass function (p.m.f.) <span class="math notranslate nohighlight">\(p(x)\)</span>, we measure the expected
amount of information through <em>entropy</em> (or <em>Shannon entropy</em>)</p>
<div class="math notranslate nohighlight" id="equation-eq-ent-def">
<span class="eqno">(22.11.3)<a class="headerlink" href="#equation-eq-ent-def" title="Permalink to this equation">¶</a></span>\[H(X) = - E_{x \sim P} [\log p(x)].\]</div>
<p>To be specific, if <span class="math notranslate nohighlight">\(X\)</span> is discrete,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-2">
<span class="eqno">(22.11.4)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-2" title="Permalink to this equation">¶</a></span>\[H(X) = - \sum_i p_i \log p_i \textrm{, where } p_i = P(X_i).\]</div>
<p>Otherwise, if <span class="math notranslate nohighlight">\(X\)</span> is continuous, we also refer entropy as
<em>differential entropy</em></p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-3">
<span class="eqno">(22.11.5)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-3" title="Permalink to this equation">¶</a></span>\[H(X) = - \int_x p(x) \log p(x) \; dx.\]</div>
<p>We can define entropy as below.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-3-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-3-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-3-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-3-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span> <span class="n">p</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">entropy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">entropy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.6855</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-3-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span> <span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">entropy</span><span class="o">.</span><span class="n">as_nd_ndarray</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">entropy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">21</span><span class="p">:</span><span class="mi">59</span><span class="p">:</span><span class="mi">56</span><span class="p">]</span> <span class="o">../</span><span class="n">src</span><span class="o">/</span><span class="n">storage</span><span class="o">/</span><span class="n">storage</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">196</span><span class="p">:</span> <span class="n">Using</span> <span class="n">Pooled</span> <span class="p">(</span><span class="n">Naive</span><span class="p">)</span> <span class="n">StorageManager</span> <span class="k">for</span> <span class="n">CPU</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">1.6854753</span><span class="p">]</span>
<span class="o">&lt;</span><span class="n">NDArray</span> <span class="mi">1</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-3-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nansum</span><span class="p">(</span><span class="o">-</span> <span class="n">p</span> <span class="o">*</span> <span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

<span class="n">entropy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=</span><span class="mf">1.6854753</span><span class="o">&gt;</span>
</pre></div>
</div>
</div></div></div>
<div class="section" id="interpretations">
<h3><span class="section-number">22.11.2.3. </span>Interpretations<a class="headerlink" href="#interpretations" title="Permalink to this heading">¶</a></h3>
<p>You may be curious: in the entropy definition <a class="reference internal" href="#equation-eq-ent-def">(22.11.3)</a>, why
do we use an expectation of a negative logarithm? Here are some
intuitions.</p>
<p>First, why do we use a <em>logarithm</em> function <span class="math notranslate nohighlight">\(\log\)</span>? Suppose that
<span class="math notranslate nohighlight">\(p(x) = f_1(x) f_2(x) \ldots, f_n(x)\)</span>, where each component
function <span class="math notranslate nohighlight">\(f_i(x)\)</span> is independent from each other. This means that
each <span class="math notranslate nohighlight">\(f_i(x)\)</span> contributes independently to the total information
obtained from <span class="math notranslate nohighlight">\(p(x)\)</span>. As discussed above, we want the entropy
formula to be additive over independent random variables. Luckily,
<span class="math notranslate nohighlight">\(\log\)</span> can naturally turn a product of probability distributions
to a summation of the individual terms.</p>
<p>Next, why do we use a <em>negative</em> <span class="math notranslate nohighlight">\(\log\)</span>? Intuitively, more
frequent events should contain less information than less common events,
since we often gain more information from an unusual case than from an
ordinary one. However, <span class="math notranslate nohighlight">\(\log\)</span> is monotonically increasing with the
probabilities, and indeed negative for all values in <span class="math notranslate nohighlight">\([0, 1]\)</span>. We
need to construct a monotonically decreasing relationship between the
probability of events and their entropy, which will ideally be always
positive (for nothing we observe should force us to forget what we have
known). Hence, we add a negative sign in front of <span class="math notranslate nohighlight">\(\log\)</span> function.</p>
<p>Last, where does the <em>expectation</em> function come from? Consider a random
variable <span class="math notranslate nohighlight">\(X\)</span>. We can interpret the self-information
(<span class="math notranslate nohighlight">\(-\log(p)\)</span>) as the amount of <em>surprise</em> we have at seeing a
particular outcome. Indeed, as the probability approaches zero, the
surprise becomes infinite. Similarly, we can interpret the entropy as
the average amount of surprise from observing <span class="math notranslate nohighlight">\(X\)</span>. For example,
imagine that a slot machine system emits statistical independently
symbols <span class="math notranslate nohighlight">\({s_1, \ldots, s_k}\)</span> with probabilities
<span class="math notranslate nohighlight">\({p_1, \ldots, p_k}\)</span> respectively. Then the entropy of this system
equals to the average self-information from observing each output, i.e.,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-4">
<span class="eqno">(22.11.6)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-4" title="Permalink to this equation">¶</a></span>\[H(S) = \sum_i {p_i \cdot I(s_i)} = - \sum_i {p_i \cdot \log p_i}.\]</div>
</div>
<div class="section" id="properties-of-entropy">
<h3><span class="section-number">22.11.2.4. </span>Properties of Entropy<a class="headerlink" href="#properties-of-entropy" title="Permalink to this heading">¶</a></h3>
<p>By the above examples and interpretations, we can derive the following
properties of entropy <a class="reference internal" href="#equation-eq-ent-def">(22.11.3)</a>. Here, we refer to <span class="math notranslate nohighlight">\(X\)</span>
as an event and <span class="math notranslate nohighlight">\(P\)</span> as the probability distribution of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H(X) \geq 0\)</span> for all discrete <span class="math notranslate nohighlight">\(X\)</span> (entropy can be
negative for continuous <span class="math notranslate nohighlight">\(X\)</span>).</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X \sim P\)</span> with a p.d.f. or a p.m.f. <span class="math notranslate nohighlight">\(p(x)\)</span>, and we
try to estimate <span class="math notranslate nohighlight">\(P\)</span> by a new probability distribution <span class="math notranslate nohighlight">\(Q\)</span>
with a p.d.f. or a p.m.f. <span class="math notranslate nohighlight">\(q(x)\)</span>, then</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-5">
<span class="eqno">(22.11.7)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-5" title="Permalink to this equation">¶</a></span>\[H(X) = - E_{x \sim P} [\log p(x)] \leq  - E_{x \sim P} [\log q(x)], \textrm{ with equality if and only if } P = Q.\]</div>
<p>Alternatively, <span class="math notranslate nohighlight">\(H(X)\)</span> gives a lower bound of the average
number of bits needed to encode symbols drawn from <span class="math notranslate nohighlight">\(P\)</span>.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(X \sim P\)</span>, then <span class="math notranslate nohighlight">\(x\)</span> conveys the maximum amount of
information if it spreads evenly among all possible outcomes.
Specifically, if the probability distribution <span class="math notranslate nohighlight">\(P\)</span> is discrete
with <span class="math notranslate nohighlight">\(k\)</span>-class <span class="math notranslate nohighlight">\(\{p_1, \ldots, p_k \}\)</span>, then</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-6">
<span class="eqno">(22.11.8)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-6" title="Permalink to this equation">¶</a></span>\[H(X) \leq \log(k), \textrm{ with equality if and only if } p_i = \frac{1}{k}, \forall i.\]</div>
<p>If <span class="math notranslate nohighlight">\(P\)</span> is a continuous random variable, then the story
becomes much more complicated. However, if we additionally impose
that <span class="math notranslate nohighlight">\(P\)</span> is supported on a finite interval (with all values
between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>), then <span class="math notranslate nohighlight">\(P\)</span> has the highest
entropy if it is the uniform distribution on that interval.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="mutual-information">
<h2><span class="section-number">22.11.3. </span>Mutual Information<a class="headerlink" href="#mutual-information" title="Permalink to this heading">¶</a></h2>
<p>Previously we defined entropy of a single random variable <span class="math notranslate nohighlight">\(X\)</span>, how
about the entropy of a pair random variables <span class="math notranslate nohighlight">\((X, Y)\)</span>? We can
think of these techniques as trying to answer the following type of
question, “What information is contained in <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>
together compared to each separately? Is there redundant information, or
is it all unique?”</p>
<p>For the following discussion, we always use <span class="math notranslate nohighlight">\((X, Y)\)</span> as a pair of
random variables that follows a joint probability distribution <span class="math notranslate nohighlight">\(P\)</span>
with a p.d.f. or a p.m.f. <span class="math notranslate nohighlight">\(p_{X, Y}(x, y)\)</span>, while <span class="math notranslate nohighlight">\(X\)</span> and
<span class="math notranslate nohighlight">\(Y\)</span> follow probability distribution <span class="math notranslate nohighlight">\(p_X(x)\)</span> and
<span class="math notranslate nohighlight">\(p_Y(y)\)</span>, respectively.</p>
<div class="section" id="joint-entropy">
<h3><span class="section-number">22.11.3.1. </span>Joint Entropy<a class="headerlink" href="#joint-entropy" title="Permalink to this heading">¶</a></h3>
<p>Similar to entropy of a single random variable <a class="reference internal" href="#equation-eq-ent-def">(22.11.3)</a>, we
define the <em>joint entropy</em> <span class="math notranslate nohighlight">\(H(X, Y)\)</span> of a pair random variables
<span class="math notranslate nohighlight">\((X, Y)\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-ent-def">
<span class="eqno">(22.11.9)<a class="headerlink" href="#equation-eq-joint-ent-def" title="Permalink to this equation">¶</a></span>\[H(X, Y) = -E_{(x, y) \sim P} [\log p_{X, Y}(x, y)].\]</div>
<p>Precisely, on the one hand, if <span class="math notranslate nohighlight">\((X, Y)\)</span> is a pair of discrete
random variables, then</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-7">
<span class="eqno">(22.11.10)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-7" title="Permalink to this equation">¶</a></span>\[H(X, Y) = - \sum_{x} \sum_{y} p_{X, Y}(x, y) \log p_{X, Y}(x, y).\]</div>
<p>On the other hand, if <span class="math notranslate nohighlight">\((X, Y)\)</span> is a pair of continuous random
variables, then we define the <em>differential joint entropy</em> as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-8">
<span class="eqno">(22.11.11)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-8" title="Permalink to this equation">¶</a></span>\[H(X, Y) = - \int_{x, y} p_{X, Y}(x, y) \ \log p_{X, Y}(x, y) \;dx \;dy.\]</div>
<p>We can think of <a class="reference internal" href="#equation-eq-joint-ent-def">(22.11.9)</a> as telling us the total
randomness in the pair of random variables. As a pair of extremes, if
<span class="math notranslate nohighlight">\(X = Y\)</span> are two identical random variables, then the information
in the pair is exactly the information in one and we have
<span class="math notranslate nohighlight">\(H(X, Y) = H(X) = H(Y)\)</span>. On the other extreme, if <span class="math notranslate nohighlight">\(X\)</span> and
<span class="math notranslate nohighlight">\(Y\)</span> are independent then <span class="math notranslate nohighlight">\(H(X, Y) = H(X) + H(Y)\)</span>. Indeed we
will always have that the information contained in a pair of random
variables is no smaller than the entropy of either random variable and
no more than the sum of both.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-9">
<span class="eqno">(22.11.12)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-9" title="Permalink to this equation">¶</a></span>\[H(X), H(Y) \le H(X, Y) \le H(X) + H(Y).\]</div>
<p>Let’s implement joint entropy from scratch.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-5-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-5-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-5-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-5-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">joint_entropy</span><span class="p">(</span><span class="n">p_xy</span><span class="p">):</span>
    <span class="n">joint_ent</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_xy</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p_xy</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">joint_ent</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">joint_entropy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.6855</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-5-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">joint_entropy</span><span class="p">(</span><span class="n">p_xy</span><span class="p">):</span>
    <span class="n">joint_ent</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_xy</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p_xy</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">joint_ent</span><span class="o">.</span><span class="n">as_nd_ndarray</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">joint_entropy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">1.6854753</span><span class="p">]</span>
<span class="o">&lt;</span><span class="n">NDArray</span> <span class="mi">1</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-5-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">joint_entropy</span><span class="p">(</span><span class="n">p_xy</span><span class="p">):</span>
    <span class="n">joint_ent</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_xy</span> <span class="o">*</span> <span class="n">log2</span><span class="p">(</span><span class="n">p_xy</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">joint_ent</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">joint_entropy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8321928</span><span class="p">,</span> <span class="mf">0.8532826</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
</div></div><p>Notice that this is the same <em>code</em> as before, but now we interpret it
differently as working on the joint distribution of the two random
variables.</p>
</div>
<div class="section" id="conditional-entropy">
<h3><span class="section-number">22.11.3.2. </span>Conditional Entropy<a class="headerlink" href="#conditional-entropy" title="Permalink to this heading">¶</a></h3>
<p>The joint entropy defined above the amount of information contained in a
pair of random variables. This is useful, but oftentimes it is not what
we care about. Consider the setting of machine learning. Let’s take
<span class="math notranslate nohighlight">\(X\)</span> to be the random variable (or vector of random variables) that
describes the pixel values of an image, and <span class="math notranslate nohighlight">\(Y\)</span> to be the random
variable which is the class label. <span class="math notranslate nohighlight">\(X\)</span> should contain substantial
information—a natural image is a complex thing. However, the information
contained in <span class="math notranslate nohighlight">\(Y\)</span> once the image has been show should be low.
Indeed, the image of a digit should already contain the information
about what digit it is unless the digit is illegible. Thus, to continue
to extend our vocabulary of information theory, we need to be able to
reason about the information content in a random variable conditional on
another.</p>
<p>In the probability theory, we saw the definition of the <em>conditional
probability</em> to measure the relationship between variables. We now want
to analogously define the <em>conditional entropy</em> <span class="math notranslate nohighlight">\(H(Y \mid X)\)</span>. We
can write this as</p>
<div class="math notranslate nohighlight" id="equation-eq-cond-ent-def">
<span class="eqno">(22.11.13)<a class="headerlink" href="#equation-eq-cond-ent-def" title="Permalink to this equation">¶</a></span>\[H(Y \mid X) = - E_{(x, y) \sim P} [\log p(y \mid x)],\]</div>
<p>where <span class="math notranslate nohighlight">\(p(y \mid x) = \frac{p_{X, Y}(x, y)}{p_X(x)}\)</span> is the
conditional probability. Specifically, if <span class="math notranslate nohighlight">\((X, Y)\)</span> is a pair of
discrete random variables, then</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-10">
<span class="eqno">(22.11.14)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-10" title="Permalink to this equation">¶</a></span>\[H(Y \mid X) = - \sum_{x} \sum_{y} p(x, y) \log p(y \mid x).\]</div>
<p>If <span class="math notranslate nohighlight">\((X, Y)\)</span> is a pair of continuous random variables, then the
<em>differential conditional entropy</em> is similarly defined as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-11">
<span class="eqno">(22.11.15)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-11" title="Permalink to this equation">¶</a></span>\[H(Y \mid X) = - \int_x \int_y p(x, y) \ \log p(y \mid x) \;dx \;dy.\]</div>
<p>It is now natural to ask, how does the <em>conditional entropy</em>
<span class="math notranslate nohighlight">\(H(Y \mid X)\)</span> relate to the entropy <span class="math notranslate nohighlight">\(H(X)\)</span> and the joint
entropy <span class="math notranslate nohighlight">\(H(X, Y)\)</span>? Using the definitions above, we can express
this cleanly:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-12">
<span class="eqno">(22.11.16)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-12" title="Permalink to this equation">¶</a></span>\[H(Y \mid X) = H(X, Y) - H(X).\]</div>
<p>This has an intuitive interpretation: the information in <span class="math notranslate nohighlight">\(Y\)</span> given
<span class="math notranslate nohighlight">\(X\)</span> (<span class="math notranslate nohighlight">\(H(Y \mid X)\)</span>) is the same as the information in both
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> together (<span class="math notranslate nohighlight">\(H(X, Y)\)</span>) minus the information
already contained in <span class="math notranslate nohighlight">\(X\)</span>. This gives us the information in
<span class="math notranslate nohighlight">\(Y\)</span> which is not also represented in <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Now, let’s implement conditional entropy <a class="reference internal" href="#equation-eq-cond-ent-def">(22.11.13)</a> from
scratch.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-7-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-7-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-7-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-7-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conditional_entropy</span><span class="p">(</span><span class="n">p_xy</span><span class="p">,</span> <span class="n">p_x</span><span class="p">):</span>
    <span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">p_xy</span><span class="o">/</span><span class="n">p_x</span>
    <span class="n">cond_ent</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_xy</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p_y_given_x</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">cond_ent</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">conditional_entropy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.8635</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-7-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conditional_entropy</span><span class="p">(</span><span class="n">p_xy</span><span class="p">,</span> <span class="n">p_x</span><span class="p">):</span>
    <span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">p_xy</span><span class="o">/</span><span class="n">p_x</span>
    <span class="n">cond_ent</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_xy</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p_y_given_x</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">cond_ent</span><span class="o">.</span><span class="n">as_nd_ndarray</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">conditional_entropy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">0.8635472</span><span class="p">]</span>
<span class="o">&lt;</span><span class="n">NDArray</span> <span class="mi">1</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-7-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conditional_entropy</span><span class="p">(</span><span class="n">p_xy</span><span class="p">,</span> <span class="n">p_x</span><span class="p">):</span>
    <span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">p_xy</span><span class="o">/</span><span class="n">p_x</span>
    <span class="n">cond_ent</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_xy</span> <span class="o">*</span> <span class="n">log2</span><span class="p">(</span><span class="n">p_y_given_x</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">cond_ent</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">conditional_entropy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=</span><span class="n">array</span><span class="p">([</span><span class="mf">0.43903595</span><span class="p">,</span> <span class="mf">0.42451128</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
</div></div></div>
<div class="section" id="id3">
<h3><span class="section-number">22.11.3.3. </span>Mutual Information<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>Given the previous setting of random variables <span class="math notranslate nohighlight">\((X, Y)\)</span>, you may
wonder: “Now that we know how much information is contained in <span class="math notranslate nohighlight">\(Y\)</span>
but not in <span class="math notranslate nohighlight">\(X\)</span>, can we similarly ask how much information is
shared between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>?” The answer will be the <em>mutual
information</em> of <span class="math notranslate nohighlight">\((X, Y)\)</span>, which we will write as <span class="math notranslate nohighlight">\(I(X, Y)\)</span>.</p>
<p>Rather than diving straight into the formal definition, let’s practice
our intuition by first trying to derive an expression for the mutual
information entirely based on terms we have constructed before. We wish
to find the information shared between two random variables. One way we
could try to do this is to start with all the information contained in
both <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> together, and then we take off the parts
that are not shared. The information contained in both <span class="math notranslate nohighlight">\(X\)</span> and
<span class="math notranslate nohighlight">\(Y\)</span> together is written as <span class="math notranslate nohighlight">\(H(X, Y)\)</span>. We want to subtract
from this the information contained in <span class="math notranslate nohighlight">\(X\)</span> but not in <span class="math notranslate nohighlight">\(Y\)</span>,
and the information contained in <span class="math notranslate nohighlight">\(Y\)</span> but not in <span class="math notranslate nohighlight">\(X\)</span>. As we
saw in the previous section, this is given by <span class="math notranslate nohighlight">\(H(X \mid Y)\)</span> and
<span class="math notranslate nohighlight">\(H(Y \mid X)\)</span> respectively. Thus, we have that the mutual
information should be</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-13">
<span class="eqno">(22.11.17)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-13" title="Permalink to this equation">¶</a></span>\[I(X, Y) = H(X, Y) - H(Y \mid X) - H(X \mid Y).\]</div>
<p>Indeed, this is a valid definition for the mutual information. If we
expand out the definitions of these terms and combine them, a little
algebra shows that this is the same as</p>
<div class="math notranslate nohighlight" id="equation-eq-mut-ent-def">
<span class="eqno">(22.11.18)<a class="headerlink" href="#equation-eq-mut-ent-def" title="Permalink to this equation">¶</a></span>\[I(X, Y) = E_{x} E_{y} \left\{ p_{X, Y}(x, y) \log\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)} \right\}.\]</div>
<p>We can summarize all of these relationships in image
<a class="reference internal" href="#fig-mutual-information"><span class="std std-numref">Fig. 22.11.1</span></a>. It is an excellent test of intuition
to see why the following statements are all also equivalent to
<span class="math notranslate nohighlight">\(I(X, Y)\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H(X) - H(X \mid Y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H(Y) - H(Y \mid X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H(X) + H(Y) - H(X, Y)\)</span></p></li>
</ul>
<div class="figure align-default" id="id5">
<span id="fig-mutual-information"></span><img alt="../_images/mutual-information.svg" src="../_images/mutual-information.svg" /><p class="caption"><span class="caption-number">Fig. 22.11.1 </span><span class="caption-text">Mutual information’s relationship with joint entropy and conditional
entropy.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>In many ways we can think of the mutual information
<a class="reference internal" href="#equation-eq-mut-ent-def">(22.11.18)</a> as principled extension of correlation
coefficient we saw in <a class="reference internal" href="random-variables.html#sec-random-variables"><span class="std std-numref">Section 22.6</span></a>. This allows us
to ask not only for linear relationships between variables, but for the
maximum information shared between the two random variables of any kind.</p>
<p>Now, let’s implement mutual information from scratch.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-9-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-9-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-9-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-9-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mutual_information</span><span class="p">(</span><span class="n">p_xy</span><span class="p">,</span> <span class="n">p_x</span><span class="p">,</span> <span class="n">p_y</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p_xy</span> <span class="o">/</span> <span class="p">(</span><span class="n">p_x</span> <span class="o">*</span> <span class="n">p_y</span><span class="p">)</span>
    <span class="n">mutual</span> <span class="o">=</span> <span class="n">p_xy</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">mutual</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">mutual_information</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span>
                   <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.7195</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-9-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mutual_information</span><span class="p">(</span><span class="n">p_xy</span><span class="p">,</span> <span class="n">p_x</span><span class="p">,</span> <span class="n">p_y</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p_xy</span> <span class="o">/</span> <span class="p">(</span><span class="n">p_x</span> <span class="o">*</span> <span class="n">p_y</span><span class="p">)</span>
    <span class="n">mutual</span> <span class="o">=</span> <span class="n">p_xy</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">mutual</span><span class="o">.</span><span class="n">as_nd_ndarray</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">mutual_information</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span>
                   <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">0.7194603</span><span class="p">]</span>
<span class="o">&lt;</span><span class="n">NDArray</span> <span class="mi">1</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-9-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mutual_information</span><span class="p">(</span><span class="n">p_xy</span><span class="p">,</span> <span class="n">p_x</span><span class="p">,</span> <span class="n">p_y</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p_xy</span> <span class="o">/</span> <span class="p">(</span><span class="n">p_x</span> <span class="o">*</span> <span class="n">p_y</span><span class="p">)</span>
    <span class="n">mutual</span> <span class="o">=</span> <span class="n">p_xy</span> <span class="o">*</span> <span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="c1"># Operator `nansum` will sum up the non-nan number</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">mutual</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">mutual_information</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=</span><span class="n">array</span><span class="p">([</span><span class="mf">0.60246783</span><span class="p">,</span> <span class="mf">0.1169925</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
</div></div></div>
<div class="section" id="properties-of-mutual-information">
<h3><span class="section-number">22.11.3.4. </span>Properties of Mutual Information<a class="headerlink" href="#properties-of-mutual-information" title="Permalink to this heading">¶</a></h3>
<p>Rather than memorizing the definition of mutual information
<a class="reference internal" href="#equation-eq-mut-ent-def">(22.11.18)</a>, you only need to keep in mind its notable
properties:</p>
<ul>
<li><p>Mutual information is symmetric, i.e., <span class="math notranslate nohighlight">\(I(X, Y) = I(Y, X)\)</span>.</p></li>
<li><p>Mutual information is non-negative, i.e., <span class="math notranslate nohighlight">\(I(X, Y) \geq 0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(I(X, Y) = 0\)</span> if and only if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are
independent. For example, if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent,
then knowing <span class="math notranslate nohighlight">\(Y\)</span> does not give any information about <span class="math notranslate nohighlight">\(X\)</span>
and vice versa, so their mutual information is zero.</p></li>
<li><p>Alternatively, if <span class="math notranslate nohighlight">\(X\)</span> is an invertible function of <span class="math notranslate nohighlight">\(Y\)</span>,
then <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> share all information and</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-14">
<span class="eqno">(22.11.19)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-14" title="Permalink to this equation">¶</a></span>\[I(X, Y) = H(Y) = H(X).\]</div>
</li>
</ul>
</div>
<div class="section" id="pointwise-mutual-information">
<h3><span class="section-number">22.11.3.5. </span>Pointwise Mutual Information<a class="headerlink" href="#pointwise-mutual-information" title="Permalink to this heading">¶</a></h3>
<p>When we worked with entropy at the beginning of this chapter, we were
able to provide an interpretation of <span class="math notranslate nohighlight">\(-\log(p_X(x))\)</span> as how
<em>surprised</em> we were with the particular outcome. We may give a similar
interpretation to the logarithmic term in the mutual information, which
is often referred to as the <em>pointwise mutual information</em>:</p>
<div class="math notranslate nohighlight" id="equation-eq-pmi-def">
<span class="eqno">(22.11.20)<a class="headerlink" href="#equation-eq-pmi-def" title="Permalink to this equation">¶</a></span>\[\textrm{pmi}(x, y) = \log\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)}.\]</div>
<p>We can think of <a class="reference internal" href="#equation-eq-pmi-def">(22.11.20)</a> as measuring how much more or less
likely the specific combination of outcomes <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are
compared to what we would expect for independent random outcomes. If it
is large and positive, then these two specific outcomes occur much more
frequently than they would compared to random chance (<em>note</em>: the
denominator is <span class="math notranslate nohighlight">\(p_X(x) p_Y(y)\)</span> which is the probability of the two
outcomes were independent), whereas if it is large and negative it
represents the two outcomes happening far less than we would expect by
random chance.</p>
<p>This allows us to interpret the mutual information
<a class="reference internal" href="#equation-eq-mut-ent-def">(22.11.18)</a> as the average amount that we were surprised
to see two outcomes occurring together compared to what we would expect
if they were independent.</p>
</div>
<div class="section" id="applications-of-mutual-information">
<h3><span class="section-number">22.11.3.6. </span>Applications of Mutual Information<a class="headerlink" href="#applications-of-mutual-information" title="Permalink to this heading">¶</a></h3>
<p>Mutual information may be a little abstract in it pure definition, so
how does it related to machine learning? In natural language processing,
one of the most difficult problems is the <em>ambiguity resolution</em>, or the
issue of the meaning of a word being unclear from context. For example,
recently a headline in the news reported that “Amazon is on fire”. You
may wonder whether the company Amazon has a building on fire, or the
Amazon rain forest is on fire.</p>
<p>In this case, mutual information can help us resolve this ambiguity. We
first find the group of words that each has a relatively large mutual
information with the company Amazon, such as e-commerce, technology, and
online. Second, we find another group of words that each has a
relatively large mutual information with the Amazon rain forest, such as
rain, forest, and tropical. When we need to disambiguate “Amazon”, we
can compare which group has more occurrence in the context of the word
Amazon. In this case the article would go on to describe the forest, and
make the context clear.</p>
</div>
</div>
<div class="section" id="kullbackleibler-divergence">
<h2><span class="section-number">22.11.4. </span>Kullback–Leibler Divergence<a class="headerlink" href="#kullbackleibler-divergence" title="Permalink to this heading">¶</a></h2>
<p>As what we have discussed in <a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#sec-linear-algebra"><span class="std std-numref">Section 2.3</span></a>, we can use
norms to measure distance between two points in space of any
dimensionality. We would like to be able to do a similar task with
probability distributions. There are many ways to go about this, but
information theory provides one of the nicest. We now explore the
<em>Kullback–Leibler (KL) divergence</em>, which provides a way to measure if
two distributions are close together or not.</p>
<div class="section" id="id4">
<h3><span class="section-number">22.11.4.1. </span>Definition<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<p>Given a random variable <span class="math notranslate nohighlight">\(X\)</span> that follows the probability
distribution <span class="math notranslate nohighlight">\(P\)</span> with a p.d.f. or a p.m.f. <span class="math notranslate nohighlight">\(p(x)\)</span>, and we
estimate <span class="math notranslate nohighlight">\(P\)</span> by another probability distribution <span class="math notranslate nohighlight">\(Q\)</span> with a
p.d.f. or a p.m.f. <span class="math notranslate nohighlight">\(q(x)\)</span>. Then the <em>Kullback–Leibler (KL)
divergence</em> (or <em>relative entropy</em>) between <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-def">
<span class="eqno">(22.11.21)<a class="headerlink" href="#equation-eq-kl-def" title="Permalink to this equation">¶</a></span>\[D_{\textrm{KL}}(P\|Q) = E_{x \sim P} \left[ \log \frac{p(x)}{q(x)} \right].\]</div>
<p>As with the pointwise mutual information <a class="reference internal" href="#equation-eq-pmi-def">(22.11.20)</a>, we can
again provide an interpretation of the logarithmic term:
<span class="math notranslate nohighlight">\(-\log \frac{q(x)}{p(x)} = -\log(q(x)) - (-\log(p(x)))\)</span> will be
large and positive if we see <span class="math notranslate nohighlight">\(x\)</span> far more often under <span class="math notranslate nohighlight">\(P\)</span>
than we would expect for <span class="math notranslate nohighlight">\(Q\)</span>, and large and negative if we see the
outcome far less than expected. In this way, we can interpret it as our
<em>relative</em> surprise at observing the outcome compared to how surprised
we would be observing it from our reference distribution.</p>
<p>Let’s implement the KL divergence from Scratch.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-11-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-11-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-11-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-11-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">kl</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span> <span class="o">/</span> <span class="n">q</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">kl</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-11-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">kl</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span> <span class="o">/</span> <span class="n">q</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">kl</span><span class="o">.</span><span class="n">as_nd_ndarray</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">asscalar</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-11-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">kl</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">log2</span><span class="p">(</span><span class="n">p</span> <span class="o">/</span> <span class="n">q</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">kl</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div></div></div>
<div class="section" id="kl-divergence-properties">
<h3><span class="section-number">22.11.4.2. </span>KL Divergence Properties<a class="headerlink" href="#kl-divergence-properties" title="Permalink to this heading">¶</a></h3>
<p>Let’s take a look at some properties of the KL divergence
<a class="reference internal" href="#equation-eq-kl-def">(22.11.21)</a>.</p>
<ul>
<li><p>KL divergence is non-symmetric, i.e., there are <span class="math notranslate nohighlight">\(P,Q\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-15">
<span class="eqno">(22.11.22)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-15" title="Permalink to this equation">¶</a></span>\[D_{\textrm{KL}}(P\|Q) \neq D_{\textrm{KL}}(Q\|P).\]</div>
</li>
<li><p>KL divergence is non-negative, i.e.,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-16">
<span class="eqno">(22.11.23)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-16" title="Permalink to this equation">¶</a></span>\[D_{\textrm{KL}}(P\|Q) \geq 0.\]</div>
<p>Note that the equality holds only when <span class="math notranslate nohighlight">\(P = Q\)</span>.</p>
</li>
<li><p>If there exists an <span class="math notranslate nohighlight">\(x\)</span> such that <span class="math notranslate nohighlight">\(p(x) &gt; 0\)</span> and
<span class="math notranslate nohighlight">\(q(x) = 0\)</span>, then <span class="math notranslate nohighlight">\(D_{\textrm{KL}}(P\|Q) = \infty\)</span>.</p></li>
<li><p>There is a close relationship between KL divergence and mutual
information. Besides the relationship shown in
<a class="reference internal" href="#fig-mutual-information"><span class="std std-numref">Fig. 22.11.1</span></a>, <span class="math notranslate nohighlight">\(I(X, Y)\)</span> is also
numerically equivalent with the following terms:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(D_{\textrm{KL}}(P(X, Y) \ \| \ P(X)P(Y))\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(E_Y \{ D_{\textrm{KL}}(P(X \mid Y) \ \| \ P(X)) \}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(E_X \{ D_{\textrm{KL}}(P(Y \mid X) \ \| \ P(Y)) \}\)</span>.</p></li>
</ol>
<p>For the first term, we interpret mutual information as the KL
divergence between <span class="math notranslate nohighlight">\(P(X, Y)\)</span> and the product of <span class="math notranslate nohighlight">\(P(X)\)</span>
and <span class="math notranslate nohighlight">\(P(Y)\)</span>, and thus is a measure of how different the joint
distribution is from the distribution if they were independent. For
the second term, mutual information tells us the average reduction in
uncertainty about <span class="math notranslate nohighlight">\(Y\)</span> that results from learning the value of
the <span class="math notranslate nohighlight">\(X\)</span>’s distribution. Similarly to the third term.</p>
</li>
</ul>
</div>
<div class="section" id="example">
<h3><span class="section-number">22.11.4.3. </span>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h3>
<p>Let’s go through a toy example to see the non-symmetry explicitly.</p>
<p>First, let’s generate and sort three tensors of length <span class="math notranslate nohighlight">\(10,000\)</span>:
an objective tensor <span class="math notranslate nohighlight">\(p\)</span> which follows a normal distribution
<span class="math notranslate nohighlight">\(N(0, 1)\)</span>, and two candidate tensors <span class="math notranslate nohighlight">\(q_1\)</span> and <span class="math notranslate nohighlight">\(q_2\)</span>
which follow normal distributions <span class="math notranslate nohighlight">\(N(-1, 1)\)</span> and <span class="math notranslate nohighlight">\(N(1, 1)\)</span>
respectively.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-13-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-13-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-13-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-13-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">tensor_len</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">tensor_len</span><span class="p">,</span> <span class="p">))</span>
<span class="n">q1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">tensor_len</span><span class="p">,</span> <span class="p">))</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">tensor_len</span><span class="p">,</span> <span class="p">))</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">p</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">q1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">q1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">q2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-13-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">nd_len</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nd_len</span><span class="p">,</span> <span class="p">))</span>
<span class="n">q1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nd_len</span><span class="p">,</span> <span class="p">))</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nd_len</span><span class="p">,</span> <span class="p">))</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()))</span>
<span class="n">q1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()))</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">q2</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()))</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-13-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor_len</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="n">tensor_len</span><span class="p">,</span> <span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">q1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="n">tensor_len</span><span class="p">,</span> <span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="n">tensor_len</span><span class="p">,</span> <span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">q1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">q1</span><span class="p">)</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">q2</span><span class="p">)</span>
</pre></div>
</div>
</div></div><p>Since <span class="math notranslate nohighlight">\(q_1\)</span> and <span class="math notranslate nohighlight">\(q_2\)</span> are symmetric with respect to the
y-axis (i.e., <span class="math notranslate nohighlight">\(x=0\)</span>), we expect a similar value of KL divergence
between <span class="math notranslate nohighlight">\(D_{\textrm{KL}}(p\|q_1)\)</span> and
<span class="math notranslate nohighlight">\(D_{\textrm{KL}}(p\|q_2)\)</span>. As you can see below, there is only a
less than 3% off between <span class="math notranslate nohighlight">\(D_{\textrm{KL}}(p\|q_1)\)</span> and
<span class="math notranslate nohighlight">\(D_{\textrm{KL}}(p\|q_2)\)</span>.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-15-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-15-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-15-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-15-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kl_pq1</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q1</span><span class="p">)</span>
<span class="n">kl_pq2</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="n">similar_percentage</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">kl_pq1</span> <span class="o">-</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">kl_pq1</span> <span class="o">+</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

<span class="n">kl_pq1</span><span class="p">,</span> <span class="n">kl_pq2</span><span class="p">,</span> <span class="n">similar_percentage</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">8582.0341796875</span><span class="p">,</span> <span class="mf">8828.3095703125</span><span class="p">,</span> <span class="mf">2.8290698237936858</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-15-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kl_pq1</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q1</span><span class="p">)</span>
<span class="n">kl_pq2</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="n">similar_percentage</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">kl_pq1</span> <span class="o">-</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">kl_pq1</span> <span class="o">+</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

<span class="n">kl_pq1</span><span class="p">,</span> <span class="n">kl_pq2</span><span class="p">,</span> <span class="n">similar_percentage</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">8470.638</span><span class="p">,</span> <span class="mf">8664.998</span><span class="p">,</span> <span class="mf">2.268492904612395</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-15-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kl_pq1</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q1</span><span class="p">)</span>
<span class="n">kl_pq2</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="n">similar_percentage</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">kl_pq1</span> <span class="o">-</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">kl_pq1</span> <span class="o">+</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

<span class="n">kl_pq1</span><span class="p">,</span> <span class="n">kl_pq2</span><span class="p">,</span> <span class="n">similar_percentage</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">8652.75</span><span class="p">,</span> <span class="mf">8690.211</span><span class="p">,</span> <span class="mf">0.43200163611047165</span><span class="p">)</span>
</pre></div>
</div>
</div></div><p>In contrast, you may find that <span class="math notranslate nohighlight">\(D_{\textrm{KL}}(q_2 \|p)\)</span> and
<span class="math notranslate nohighlight">\(D_{\textrm{KL}}(p \| q_2)\)</span> are off a lot, with around 40% off as
shown below.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-17-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-17-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-17-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-17-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kl_q2p</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">q2</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">differ_percentage</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">kl_q2p</span> <span class="o">-</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">kl_q2p</span> <span class="o">+</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

<span class="n">kl_q2p</span><span class="p">,</span> <span class="n">differ_percentage</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">14130.125</span><span class="p">,</span> <span class="mf">46.18621024399691</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-17-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kl_q2p</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">q2</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">differ_percentage</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">kl_q2p</span> <span class="o">-</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">kl_q2p</span> <span class="o">+</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

<span class="n">kl_q2p</span><span class="p">,</span> <span class="n">differ_percentage</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">13536.835</span><span class="p">,</span> <span class="mf">43.88680093791528</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-17-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kl_q2p</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">q2</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">differ_percentage</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">kl_q2p</span> <span class="o">-</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">kl_q2p</span> <span class="o">+</span> <span class="n">kl_pq2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

<span class="n">kl_q2p</span><span class="p">,</span> <span class="n">differ_percentage</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">13416.902</span><span class="p">,</span> <span class="mf">42.761724211717066</span><span class="p">)</span>
</pre></div>
</div>
</div></div></div>
</div>
<div class="section" id="cross-entropy">
<h2><span class="section-number">22.11.5. </span>Cross-Entropy<a class="headerlink" href="#cross-entropy" title="Permalink to this heading">¶</a></h2>
<p>If you are curious about applications of information theory in deep
learning, here is a quick example. We define the true distribution
<span class="math notranslate nohighlight">\(P\)</span> with probability distribution <span class="math notranslate nohighlight">\(p(x)\)</span>, and the estimated
distribution <span class="math notranslate nohighlight">\(Q\)</span> with probability distribution <span class="math notranslate nohighlight">\(q(x)\)</span>, and
we will use them in the rest of this section.</p>
<p>Say we need to solve a binary classification problem based on given
<span class="math notranslate nohighlight">\(n\)</span> data examples {<span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>}. Assume that we
encode <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(0\)</span> as the positive and negative class label
<span class="math notranslate nohighlight">\(y_i\)</span> respectively, and our neural network is parametrized by
<span class="math notranslate nohighlight">\(\theta\)</span>. If we aim to find a best <span class="math notranslate nohighlight">\(\theta\)</span> so that
<span class="math notranslate nohighlight">\(\hat{y}_i= p_{\theta}(y_i \mid x_i)\)</span>, it is natural to apply the
maximum log-likelihood approach as was seen in
<a class="reference internal" href="maximum-likelihood.html#sec-maximum-likelihood"><span class="std std-numref">Section 22.7</span></a>. To be specific, for true labels
<span class="math notranslate nohighlight">\(y_i\)</span> and predictions <span class="math notranslate nohighlight">\(\hat{y}_i= p_{\theta}(y_i \mid x_i)\)</span>,
the probability to be classified as positive is
<span class="math notranslate nohighlight">\(\pi_i= p_{\theta}(y_i = 1 \mid x_i)\)</span>. Hence, the log-likelihood
function would be</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-17">
<span class="eqno">(22.11.24)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-17" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
l(\theta) &amp;= \log L(\theta) \\
  &amp;= \log \prod_{i=1}^n \pi_i^{y_i} (1 - \pi_i)^{1 - y_i} \\
  &amp;= \sum_{i=1}^n y_i \log(\pi_i) + (1 - y_i) \log (1 - \pi_i). \\
\end{aligned}\end{split}\]</div>
<p>Maximizing the log-likelihood function <span class="math notranslate nohighlight">\(l(\theta)\)</span> is identical to
minimizing <span class="math notranslate nohighlight">\(- l(\theta)\)</span>, and hence we can find the best
<span class="math notranslate nohighlight">\(\theta\)</span> from here. To generalize the above loss to any
distributions, we also called <span class="math notranslate nohighlight">\(-l(\theta)\)</span> the <em>cross-entropy
loss</em> <span class="math notranslate nohighlight">\(\textrm{CE}(y, \hat{y})\)</span>, where <span class="math notranslate nohighlight">\(y\)</span> follows the true
distribution <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(\hat{y}\)</span> follows the estimated
distribution <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>This was all derived by working from the maximum likelihood point of
view. However, if we look closely we can see that terms like
<span class="math notranslate nohighlight">\(\log(\pi_i)\)</span> have entered into our computation which is a solid
indication that we can understand the expression from an information
theoretic point of view.</p>
<div class="section" id="formal-definition">
<h3><span class="section-number">22.11.5.1. </span>Formal Definition<a class="headerlink" href="#formal-definition" title="Permalink to this heading">¶</a></h3>
<p>Like KL divergence, for a random variable <span class="math notranslate nohighlight">\(X\)</span>, we can also measure
the divergence between the estimating distribution <span class="math notranslate nohighlight">\(Q\)</span> and the
true distribution <span class="math notranslate nohighlight">\(P\)</span> via <em>cross-entropy</em>,</p>
<div class="math notranslate nohighlight" id="equation-eq-ce-def">
<span class="eqno">(22.11.25)<a class="headerlink" href="#equation-eq-ce-def" title="Permalink to this equation">¶</a></span>\[\textrm{CE}(P, Q) = - E_{x \sim P} [\log(q(x))].\]</div>
<p>By using properties of entropy discussed above, we can also interpret it
as the summation of the entropy <span class="math notranslate nohighlight">\(H(P)\)</span> and the KL divergence
between <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, i.e.,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-18">
<span class="eqno">(22.11.26)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-18" title="Permalink to this equation">¶</a></span>\[\textrm{CE} (P, Q) = H(P) + D_{\textrm{KL}}(P\|Q).\]</div>
<p>We can implement the cross-entropy loss as below.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-19-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-19-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-19-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-19-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">ce</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)),</span> <span class="n">y</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ce</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-19-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">ce</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)),</span> <span class="n">y</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ce</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-19-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># `tf.gather_nd` is used to select specific indices of a tensor.</span>
    <span class="n">ce</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="p">[[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)),</span> <span class="n">y</span><span class="p">)]))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">ce</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div></div><p>Now define two tensors for the labels and predictions, and calculate the
cross-entropy loss of them.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-21-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-21-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-21-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-21-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>

<span class="n">cross_entropy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.9486</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-21-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>

<span class="n">cross_entropy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="mf">0.94856</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-21-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>

<span class="n">cross_entropy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.94856</span>
</pre></div>
</div>
</div></div></div>
<div class="section" id="properties">
<h3><span class="section-number">22.11.5.2. </span>Properties<a class="headerlink" href="#properties" title="Permalink to this heading">¶</a></h3>
<p>As alluded in the beginning of this section, cross-entropy
<a class="reference internal" href="#equation-eq-ce-def">(22.11.25)</a> can be used to define a loss function in the
optimization problem. It turns out that the following are equivalent:</p>
<ol class="arabic simple">
<li><p>Maximizing predictive probability of <span class="math notranslate nohighlight">\(Q\)</span> for distribution
<span class="math notranslate nohighlight">\(P\)</span>, (i.e., <span class="math notranslate nohighlight">\(E_{x \sim P} [\log (q(x))]\)</span>);</p></li>
<li><p>Minimizing cross-entropy <span class="math notranslate nohighlight">\(\textrm{CE} (P, Q)\)</span>;</p></li>
<li><p>Minimizing the KL divergence <span class="math notranslate nohighlight">\(D_{\textrm{KL}}(P\|Q)\)</span>.</p></li>
</ol>
<p>The definition of cross-entropy indirectly proves the equivalent
relationship between objective 2 and objective 3, as long as the entropy
of true data <span class="math notranslate nohighlight">\(H(P)\)</span> is constant.</p>
</div>
<div class="section" id="cross-entropy-as-an-objective-function-of-multi-class-classification">
<h3><span class="section-number">22.11.5.3. </span>Cross-Entropy as An Objective Function of Multi-class Classification<a class="headerlink" href="#cross-entropy-as-an-objective-function-of-multi-class-classification" title="Permalink to this heading">¶</a></h3>
<p>If we dive deep into the classification objective function with
cross-entropy loss <span class="math notranslate nohighlight">\(\textrm{CE}\)</span>, we will find minimizing
<span class="math notranslate nohighlight">\(\textrm{CE}\)</span> is equivalent to maximizing the log-likelihood
function <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>To begin with, suppose that we are given a dataset with <span class="math notranslate nohighlight">\(n\)</span>
examples, and it can be classified into <span class="math notranslate nohighlight">\(k\)</span>-classes. For each data
example <span class="math notranslate nohighlight">\(i\)</span>, we represent any <span class="math notranslate nohighlight">\(k\)</span>-class label
<span class="math notranslate nohighlight">\(\mathbf{y}_i = (y_{i1}, \ldots, y_{ik})\)</span> by <em>one-hot encoding</em>.
To be specific, if the example <span class="math notranslate nohighlight">\(i\)</span> belongs to class <span class="math notranslate nohighlight">\(j\)</span>,
then we set the <span class="math notranslate nohighlight">\(j\)</span>-th entry to <span class="math notranslate nohighlight">\(1\)</span>, and all other
components to <span class="math notranslate nohighlight">\(0\)</span>, i.e.,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-19">
<span class="eqno">(22.11.27)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-19" title="Permalink to this equation">¶</a></span>\[\begin{split}y_{ij} = \begin{cases}1 &amp; j \in J; \\ 0 &amp;\textrm{otherwise.}\end{cases}\end{split}\]</div>
<p>For instance, if a multi-class classification problem contains three
classes <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span>, then the labels
<span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> can be encoded in
{<span class="math notranslate nohighlight">\(A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)\)</span>}.</p>
<p>Assume that our neural network is parametrized by <span class="math notranslate nohighlight">\(\theta\)</span>. For
true label vectors <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> and predictions</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-20">
<span class="eqno">(22.11.28)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-20" title="Permalink to this equation">¶</a></span>\[\hat{\mathbf{y}}_i= p_{\theta}(\mathbf{y}_i \mid \mathbf{x}_i) = \sum_{j=1}^k y_{ij} p_{\theta} (y_{ij}  \mid  \mathbf{x}_i).\]</div>
<p>Hence, the <em>cross-entropy loss</em> would be</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-21">
<span class="eqno">(22.11.29)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-21" title="Permalink to this equation">¶</a></span>\[\begin{split}\textrm{CE}(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^n \mathbf{y}_i \log \hat{\mathbf{y}}_i
 = - \sum_{i=1}^n \sum_{j=1}^k y_{ij} \log{p_{\theta} (y_{ij}  \mid  \mathbf{x}_i)}.\\\end{split}\]</div>
<p>On the other side, we can also approach the problem through maximum
likelihood estimation. To begin with, let’s quickly introduce a
<span class="math notranslate nohighlight">\(k\)</span>-class multinoulli distribution. It is an extension of the
Bernoulli distribution from binary class to multi-class. If a random
variable <span class="math notranslate nohighlight">\(\mathbf{z} = (z_{1}, \ldots, z_{k})\)</span> follows a
<span class="math notranslate nohighlight">\(k\)</span>-class <em>multinoulli distribution</em> with probabilities
<span class="math notranslate nohighlight">\(\mathbf{p} =\)</span> (<span class="math notranslate nohighlight">\(p_{1}, \ldots, p_{k}\)</span>), i.e.,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-22">
<span class="eqno">(22.11.30)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-22" title="Permalink to this equation">¶</a></span>\[p(\mathbf{z}) = p(z_1, \ldots, z_k) = \textrm{Multi} (p_1, \ldots, p_k), \textrm{ where } \sum_{i=1}^k p_i = 1,\]</div>
<p>then the joint probability mass function(p.m.f.) of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>
is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-23">
<span class="eqno">(22.11.31)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-23" title="Permalink to this equation">¶</a></span>\[\mathbf{p}^\mathbf{z} = \prod_{j=1}^k p_{j}^{z_{j}}.\]</div>
<p>It can be seen that the label of each data example,
<span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>, is following a <span class="math notranslate nohighlight">\(k\)</span>-class multinoulli
distribution with probabilities <span class="math notranslate nohighlight">\(\boldsymbol{\pi} =\)</span>
(<span class="math notranslate nohighlight">\(\pi_{1}, \ldots, \pi_{k}\)</span>). Therefore, the joint p.m.f. of each
data example <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> is
<span class="math notranslate nohighlight">\(\mathbf{\pi}^{\mathbf{y}_i} = \prod_{j=1}^k \pi_{j}^{y_{ij}}.\)</span>
Hence, the log-likelihood function would be</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-information-theory-24">
<span class="eqno">(22.11.32)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-information-theory-24" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
l(\theta)
 = \log L(\theta)
 = \log \prod_{i=1}^n \boldsymbol{\pi}^{\mathbf{y}_i}
 = \log \prod_{i=1}^n \prod_{j=1}^k \pi_{j}^{y_{ij}}
 = \sum_{i=1}^n \sum_{j=1}^k y_{ij} \log{\pi_{j}}.\\
\end{aligned}\end{split}\]</div>
<p>Since in maximum likelihood estimation, we maximizing the objective
function <span class="math notranslate nohighlight">\(l(\theta)\)</span> by having
<span class="math notranslate nohighlight">\(\pi_{j} = p_{\theta} (y_{ij} \mid \mathbf{x}_i)\)</span>. Therefore, for
any multi-class classification, maximizing the above log-likelihood
function <span class="math notranslate nohighlight">\(l(\theta)\)</span> is equivalent to minimizing the CE loss
<span class="math notranslate nohighlight">\(\textrm{CE}(y, \hat{y})\)</span>.</p>
<p>To test the above proof, let’s apply the built-in measure
<code class="docutils literal notranslate"><span class="pre">NegativeLogLikelihood</span></code>. Using the same <code class="docutils literal notranslate"><span class="pre">labels</span></code> and <code class="docutils literal notranslate"><span class="pre">preds</span></code> as in
the earlier example, we will get the same numerical loss as the previous
example up to the 5 decimal place.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-23-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-23-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-23-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-23-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`</span>
<span class="c1"># and `nn.NLLLoss()`</span>
<span class="n">nll_loss</span> <span class="o">=</span> <span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">preds</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.9486</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-23-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nll_loss</span> <span class="o">=</span> <span class="n">NegativeLogLikelihood</span><span class="p">()</span>
<span class="n">nll_loss</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">as_nd_ndarray</span><span class="p">(),</span> <span class="n">preds</span><span class="o">.</span><span class="n">as_nd_ndarray</span><span class="p">())</span>
<span class="n">nll_loss</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="s1">&#39;nll-loss&#39;</span><span class="p">,</span> <span class="mf">0.9485599994659424</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-23-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Convert labels to one-hot vectors.</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># We will not calculate negative log-likelihood from the definition.</span>
    <span class="c1"># Rather, we will follow a circular argument. Because NLL is same as</span>
    <span class="c1"># `cross_entropy`, if we calculate cross_entropy that would give us NLL</span>
    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">(</span>
        <span class="n">from_logits</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">preds</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.94856</span>
</pre></div>
</div>
</div></div></div>
</div>
<div class="section" id="summary">
<h2><span class="section-number">22.11.6. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Information theory is a field of study about encoding, decoding,
transmitting, and manipulating information.</p></li>
<li><p>Entropy is the unit to measure how much information is presented in
different signals.</p></li>
<li><p>KL divergence can also measure the divergence between two
distributions.</p></li>
<li><p>Cross-entropy can be viewed as an objective function of multi-class
classification. Minimizing cross-entropy loss is equivalent to
maximizing the log-likelihood function.</p></li>
</ul>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">22.11.7. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>Verify that the card examples from the first section indeed have the
claimed entropy.</p></li>
<li><p>Show that the KL divergence <span class="math notranslate nohighlight">\(D(p\|q)\)</span> is nonnegative for all
distributions <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>. Hint: use Jensen’s inequality,
i.e., use the fact that <span class="math notranslate nohighlight">\(-\log x\)</span> is a convex function.</p></li>
<li><p>Let’s compute the entropy from a few data sources:</p>
<ul class="simple">
<li><p>Assume that you are watching the output generated by a monkey at a
typewriter. The monkey presses any of the <span class="math notranslate nohighlight">\(44\)</span> keys of the
typewriter at random (you can assume that it has not discovered
any special keys or the shift key yet). How many bits of
randomness per character do you observe?</p></li>
<li><p>Being unhappy with the monkey, you replaced it by a drunk
typesetter. It is able to generate words, albeit not coherently.
Instead, it picks a random word out of a vocabulary of
<span class="math notranslate nohighlight">\(2,000\)</span> words. Let’s assume that the average length of a
word is <span class="math notranslate nohighlight">\(4.5\)</span> letters in English. How many bits of
randomness per character do you observe now?</p></li>
<li><p>Still being unhappy with the result, you replace the typesetter by
a high quality language model. The language model can currently
obtain a perplexity as low as <span class="math notranslate nohighlight">\(15\)</span> points per word. The
character <em>perplexity</em> of a language model is defined as the
inverse of the geometric mean of a set of probabilities, each
probability is corresponding to a character in the word. To be
specific, if the length of a given word is <span class="math notranslate nohighlight">\(l\)</span>, then
<span class="math notranslate nohighlight">\(\textrm{PPL}(\textrm{word}) = \left[\prod_i p(\textrm{character}_i)\right]^{ -\frac{1}{l}} = \exp \left[ - \frac{1}{l} \sum_i{\log p(\textrm{character}_i)} \right].\)</span>
Assume that the test word has 4.5 letters, how many bits of
randomness per character do you observe now?</p></li>
</ul>
</li>
<li><p>Explain intuitively why <span class="math notranslate nohighlight">\(I(X, Y) = H(X) - H(X \mid Y)\)</span>. Then,
show this is true by expressing both sides as an expectation with
respect to the joint distribution.</p></li>
<li><p>What is the KL Divergence between the two Gaussian distributions
<span class="math notranslate nohighlight">\(\mathcal{N}(\mu_1, \sigma_1^2)\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{N}(\mu_2, \sigma_2^2)\)</span>?</p></li>
</ol>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar text"><a href="#pytorch-25-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-25-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-25-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-25-0"><p><a class="reference external" href="https://discuss.d2l.ai/t/1104">Discussions</a></p>
</div><div class="mdl-tabs__panel " id="mxnet-25-1"><p><a class="reference external" href="https://discuss.d2l.ai/t/420">Discussions</a></p>
</div><div class="mdl-tabs__panel " id="tensorflow-25-2"><p><a class="reference external" href="https://discuss.d2l.ai/t/1105">Discussions</a></p>
</div></div></div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">22.11. Information Theory</a><ul>
<li><a class="reference internal" href="#information">22.11.1. Information</a><ul>
<li><a class="reference internal" href="#self-information">22.11.1.1. Self-information</a></li>
</ul>
</li>
<li><a class="reference internal" href="#entropy">22.11.2. Entropy</a><ul>
<li><a class="reference internal" href="#motivating-entropy">22.11.2.1. Motivating Entropy</a></li>
<li><a class="reference internal" href="#definition">22.11.2.2. Definition</a></li>
<li><a class="reference internal" href="#interpretations">22.11.2.3. Interpretations</a></li>
<li><a class="reference internal" href="#properties-of-entropy">22.11.2.4. Properties of Entropy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mutual-information">22.11.3. Mutual Information</a><ul>
<li><a class="reference internal" href="#joint-entropy">22.11.3.1. Joint Entropy</a></li>
<li><a class="reference internal" href="#conditional-entropy">22.11.3.2. Conditional Entropy</a></li>
<li><a class="reference internal" href="#id3">22.11.3.3. Mutual Information</a></li>
<li><a class="reference internal" href="#properties-of-mutual-information">22.11.3.4. Properties of Mutual Information</a></li>
<li><a class="reference internal" href="#pointwise-mutual-information">22.11.3.5. Pointwise Mutual Information</a></li>
<li><a class="reference internal" href="#applications-of-mutual-information">22.11.3.6. Applications of Mutual Information</a></li>
</ul>
</li>
<li><a class="reference internal" href="#kullbackleibler-divergence">22.11.4. Kullback–Leibler Divergence</a><ul>
<li><a class="reference internal" href="#id4">22.11.4.1. Definition</a></li>
<li><a class="reference internal" href="#kl-divergence-properties">22.11.4.2. KL Divergence Properties</a></li>
<li><a class="reference internal" href="#example">22.11.4.3. Example</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cross-entropy">22.11.5. Cross-Entropy</a><ul>
<li><a class="reference internal" href="#formal-definition">22.11.5.1. Formal Definition</a></li>
<li><a class="reference internal" href="#properties">22.11.5.2. Properties</a></li>
<li><a class="reference internal" href="#cross-entropy-as-an-objective-function-of-multi-class-classification">22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification</a></li>
</ul>
</li>
<li><a class="reference internal" href="#summary">22.11.6. Summary</a></li>
<li><a class="reference internal" href="#exercises">22.11.7. Exercises</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="statistics.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>22.10. Statistics</div>
         </div>
     </a>
     <a id="button-next" href="../chapter_appendix-tools-for-deep-learning/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>23. Appendix: Tools for Deep Learning</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>