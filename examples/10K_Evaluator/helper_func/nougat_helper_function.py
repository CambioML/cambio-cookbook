from uniflow.op.prompt import Context

def process_content(content, client_openAI):
    """
    Overview:
    The `process_content` function is tailored for processing markdown content,
    especially useful for handling large sections and tables extracted by the `ExtractClient`.
    It efficiently manages content by dividing it into smaller sections, refines table data,
    and optionally utilizes OpenAI for advanced content processing tasks.

    Inputs:
    - `content`: String content generated by `ExtractClient`.
    - `client_openAI`: An object representing the OpenAI client for processing markdown sections.

    Workflow:
    1. Reading the File: Starts by reading the content of the markdown file.
    2. Initial Splitting: Divides the content into sections based on '##' headers, skipping the first if empty.
    3. Sub-Splitting for Large Sections: Further divides sections exceeding `max_word_count` using '###' headers.
    4. Processing for Table Content: Reduces word count in sections with extensive table content (`word count > max_word_count_for_table`) while keeping essential information.
    5. Word Count Reduction Check: If a section's word count falls below `reduction_threshold` post-processing, it undergoes further refinement via the OpenAI client.
    6. Compilation of Processed Sections: Gathers all non-empty, processed sections into a list.
    7. Statistics: Reports the number of sections that were further split and those significantly reduced in word count.

    Output:
    Returns a list of strings, each representing a processed section of the original markdown content, cleaned and potentially refined with AI processing.

    Note:
    Post-table processing, some sections might contain only headers. To ensure the output's relevance, consider excluding these by setting a minimum length requirement for each section.
    """
    # Constants and counters
    max_word_count_for_table = 25
    max_word_count = 4096
    reduction_threshold = 0.30
    further_splitted_count = 0
    significantly_reduced_count = 0

    # Splitting the content
    sections = content.split('##')
    intermediate_sections = []

    for i, section in enumerate(sections):
        if i == 0 and not section.strip():
            continue

        # Add '##' back to the section header
        if not section.lstrip().startswith('#'):
            section = '##' + section

        # Split large sections using '###'
        if len(section.split()) > max_word_count:
            sub_sections = section.split('###')
            for sub_section in sub_sections:
                if len(sub_section.split()) > max_word_count:
                    further_splitted_sub_sections = split_large_section(sub_section, max_word_count)
                    further_splitted_count += len(further_splitted_sub_sections) - 1
                    intermediate_sections.extend(further_splitted_sub_sections)
                else:
                    intermediate_sections.append(sub_section)
        else:
            intermediate_sections.append(section)

    # Process each section for table content and check word count reduction
    cleaned_sections = []
    for section in intermediate_sections:
        original_word_count = len(section.split())
        processed_section = process_for_table_content(section, max_word_count_for_table)

        # Calculate word count reduction
        processed_word_count = len(processed_section.split())
        if processed_word_count == 0 or processed_word_count / original_word_count < reduction_threshold:
            significantly_reduced_count += 1
            # Use OpenAI-based processing for sections that are significantly reduced
            temp_processed_section = clean_text_from_table_syntax_with_openAI(section, client_openAI)
            if temp_processed_section:
                processed_section = temp_processed_section

        if processed_section:
            cleaned_sections.append(processed_section)

    print(f"Number of chunks further split: {further_splitted_count}")
    print(f"Number of significantly reduced chunks: {significantly_reduced_count}")

    return cleaned_sections

def split_large_section(section, max_word_count):
    """
    Overview:
    The `split_large_section` function is tasked with dividing a large text section into smaller, more manageable chunks,
    adhering to a specified maximum word count. This is particularly useful for enhancing the readability of large text blocks
    or meeting specific processing requirements that necessitate smaller segments of text.

    Inputs:
    - `section`: A string representing the text section to be divided.
    - `max_word_count`: An integer that defines the maximum number of words allowed in each chunk.

    Output:
    Returns a list of strings, with each string being a chunk of the original text section.
    Each chunk is carefully constructed to contain words up to the `max_word_count`, ensuring
    that no chunk surpasses this predetermined word limit.
    """
    words = section.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) > max_word_count:
            chunks.append(' '.join(current_chunk[:-1]))
            current_chunk = [word]

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

def process_for_table_content(section, max_word_count_for_table):
    """
    Overview:
    The `process_for_table_content` function is tailored for filtering and processing text sections that are structured similarly to tables.
    Its primary goal is to preserve meaningful content while adhering to a specified maximum word count for each processed chunk,
    making it highly suitable for refining markdown content with table-like structures for readability or further analysis.

    Inputs:
    - `section`: A string that represents the text section to undergo processing. The content is expected to be in markdown format,
      often containing data or information presented in a table-like manner.
    - `max_word_count_for_table`: An integer that determines the maximum word count allowed for each chunk derived from the table-like content,
      ensuring that each processed chunk remains concise and within the specified limit.

    Output:
    Returns a string that encapsulates the processed section. This output string consists of carefully filtered lines that not only meet
    the established word count criteria but also avoid markdown header lines, thereby ensuring the retention of relevant content in a
    more digestible and structured format.
    """
    lines = [line for line in section.split('\n') if line.strip() and not line.strip().startswith('##') and not line.strip().startswith('###')]
    filtered_lines = []
    i = 0

    while i < len(lines):
        end_index = min(i + 4, len(lines))
        word_count = sum(len(line.split()) for line in lines[i:end_index])

        if word_count >= max_word_count_for_table or end_index - i < 4:
            filtered_lines.extend(lines[i:end_index])
        i = end_index

    return '\n'.join(filtered_lines).strip()

def clean_text_from_table_syntax_with_openAI(text_chunk, client_openAI):
    """
    Overview:
    The `clean_text_from_table_syntax_with_openAI` function is engineered to refine text chunks by cleaning and reformatting text that contains
    table-like syntax, utilizing the OpenAI API for sophisticated processing capabilities. This function excels in transforming and simplifying
    complex text structures, making it particularly valuable for processing and enhancing text with dense or structured data presentations.

    Inputs:
    - `text_chunk`: A string that encapsulates the text chunk targeted for processing. The chunk is often complex or structured similarly to a table,
      necessitating advanced processing for clarity and simplification.
    - `client_openAI`: An object representing the OpenAI client, which is employed to analyze and process the specified text chunk, leveraging
      OpenAI's capabilities to interpret and clean table-like content effectively.

    Output:
    - Returns the processed text as a string, provided a valid 'cleaned_context' can be derived from the OpenAI client's response, signifying
      that the text has been successfully cleaned and reformatted.
    - Returns an empty list in scenarios where the input does not conform to expectations, or when the OpenAI client's response does not contain
      the necessary data for a successful cleaning process.
    """
    # Validate input
    if not isinstance(text_chunk, str):
        return []

    input_data = [Context(context=text_chunk)]
    output_openAI = client_openAI.run(input_data)

    # Check if 'output' is in the first item of the output_openAI list
    if isinstance(output_openAI, list) and len(output_openAI) > 0 and 'output' in output_openAI[0]:
        first_output = output_openAI[0]['output']

        # Check if first_output is a list and not empty
        if isinstance(first_output, list) and len(first_output) > 0:
            first_response = first_output[0]

            # Check if 'response' is in the first_response and it's not empty
            if isinstance(first_response, dict) and 'response' in first_response and isinstance(first_response['response'], list) and len(first_response['response']) > 0:
                first_responses = first_response['response'][0]

                # Check if 'responses' is in first_responses and it has at least two elements
                if isinstance(first_responses, dict) and 'responses' in first_responses and isinstance(first_responses['responses'], list) and len(first_responses['responses']) > 1:
                    cleaned_context = first_responses['responses'][1].get('cleaned_context')

                    # Check if cleaned_context is not None
                    if cleaned_context is not None:
                        return cleaned_context

    return []  # Return an empty list if the conditions are not met

if __name__ == "__main__":
    print("Hello World!")