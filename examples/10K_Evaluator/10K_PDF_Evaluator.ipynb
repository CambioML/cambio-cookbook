{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "7cbc4c4a",
      "metadata": {},
      "source": [
       "# Build Your Own 10K Agent: Transform a Unstructured Financial Report to an Finetuned LLM\n",
       "Do you want to build an agent so that you can ask it anything about the annual report (10K)? In this example, we will show you how use `uniflow` and `pykoi` to extract knowledge from a unstructured annual report (10K) and then finetune an LLM on these knowledge.\n",
       "\n",
       "First, we'll use `uniflow` to generate question-answers (QAs) from a pdf using OpenAI's models via `uniflow`'s `MultiFlowPipeline`.\n",
       "\n",
       "Next, we'll use `pykoi` to run supervised fine-tuning (SFT) on the QAs generated by `uniflow`.\n",
       "\n",
       "Finally, we'll use `pykoi`'s Chatbot to run the SFT model, so you can ask questions about the 10K and get answers.\n",
       "\n",
       "For this example, we're using a 10K from [Nike](https://investors.nike.com/investors/news-events-and-reports/), [Amazon](https://ir.aboutamazon.com/sec-filings/sec-filings-details/default.aspx?FilingId=16361618), and [Alphabet](https://abc.xyz/investor/sec-filings/annual-filings/2023/).\n",
       "\n",
       ">*Note: In order to run this notebook, you need a GPU (for the `RLHF`).*\n",
       "\n",
       "### Before running the code\n",
       "\n",
       "You will need to set up a conda environment to run this notebook. You can set up the environment following the [instruction](https://github.com/CambioML/cambio-recipes/tree/main#installation).\n",
       "\n",
       "We are using uniflow and several of the pykoi modules, so you will need to install these in your environment as well:\n",
       "```\n",
       "pip3 install uniflow\n",
       "pip3 install \"pykoi[huggingface, rag, rlhf]\"\n",
       "```\n",
       "Finally, you will need to install torch:\n",
       "```\n",
       "pip3 uninstall torch\n",
       "pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121  # cu121 means cuda 12.1\n",
       "```\n",
       "\n",
       "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/cambio-recipes/tree/main#api-keys)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Generate QAs from a 10K using `uniflow`"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Update System Path"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "%reload_ext autoreload\n",
       "%autoreload 2\n",
       "\n",
       "import sys\n",
       "\n",
       "sys.path.append(\".\")\n",
       "sys.path.append(\"..\")\n",
       "sys.path.append(\"../..\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Install helper packages\n",
       "If you already have these installed, feel free to skip this step."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "!{sys.executable} -m pip install pandas"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Import Dependency"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d84dd70",
      "metadata": {},
      "outputs": [],
      "source": [
       "from dotenv import load_dotenv\n",
       "import os\n",
       "import pandas as pd\n",
       "\n",
       "from uniflow.pipeline import MultiFlowsPipeline\n",
       "from uniflow.flow.config import PipelineConfig\n",
       "from uniflow.flow.config import TransformOpenAIConfig, ExtractPDFConfig\n",
       "from uniflow.op.model.model_config import OpenAIModelConfig, NougatModelConfig\n",
       "from uniflow.op.prompt import PromptTemplate, Context\n",
       "from uniflow.op.extract.split.constants import PARAGRAPH_SPLITTER\n",
       "\n",
       "load_dotenv()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "cb677037",
      "metadata": {},
      "source": [
       "### Prepare the input data\n",
       "First, uncomment the 10k that you want to use."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "a707ef78",
      "metadata": {},
      "outputs": [],
      "source": [
       "pdf_file = \"nike-10k-2023.pdf\"\n",
       "# pdf_file = \"amazon-10k-2023.pdf\"\n",
       "# pdf_file = \"alphabet-10k-2023.pdf\""
      ]
     },
     {
      "cell_type": "markdown",
      "id": "4b177df1",
      "metadata": {},
      "source": [
       "##### Set current directory and input data directory."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "092b355a",
      "metadata": {},
      "outputs": [],
      "source": [
       "dir_cur = os.getcwd()\n",
       "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", pdf_file)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Load the pdf using Nougat\n",
       "For this example, we'll run the `ExtractPDF` flow to extract the text from the 10K pdf. This uses the [Nougat](https://pypi.org/project/nougat-ocr/0.1.17/) PDF parser."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "data = [\n",
       "    {\"pdf\": input_file},\n",
       "]\n",
       "\n",
       "extract_config = ExtractPDFConfig(\n",
       "    model_config=NougatModelConfig(\n",
       "        batch_size = 4 # When batch_size>1, nougat will run on CUDA, otherwise it will run on CPU\n",
       "    ),\n",
       "    splitter=PARAGRAPH_SPLITTER,\n",
       ")\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Now we need to write a little bit prompts to generate question and answer for a given paragraph, each promopt data includes a instruction and a list of examples with \"context\", \"question\" and \"answer\"."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "prompt_template = PromptTemplate(\n",
       "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
       "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
       "    few_shot_prompt=[\n",
       "        Context(\n",
       "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
       "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
       "            answer=\"Claude E. Shannon.\",\n",
       "        ),\n",
       "])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Use LLM to generate data\n",
       "\n",
       "In this example, we will use the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17)'s default LLM to generate questions and answers."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "transform_config = TransformOpenAIConfig()\n",
       "transform_config.prompt_template = prompt_template"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0fa16272",
      "metadata": {},
      "source": [
       "If we want the response format to be JSON, we need to update two aspects of the default config:\n",
       "\n",
       "1. Change the model_name to \"gpt-4-1106-preview\", which is the only GPT-4 model that supports the JSON format.\n",
       "1. Change the response_format to a json_object."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "092d899f",
      "metadata": {},
      "outputs": [],
      "source": [
       "transform_config.model_config.model_name = \"gpt-4-1106-preview\"\n",
       "transform_config.model_config.response_format = {\"type\": \"json_object\"}\n",
       "transform_config.model_config.num_call = 1\n",
       "transform_config.model_config.temperature = 0.0"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "4d81cfc7",
      "metadata": {},
      "source": [
       "Finally, we update the `num_threads` and `batch_size`. You'll want to optimize this number to maximize efficiency. Note that these must be the same number."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d0515b",
      "metadata": {},
      "outputs": [],
      "source": [
       "from pprint import pprint\n",
       "\n",
       "num_thread_batch_size = 32\n",
       "transform_config.model_config.num_thread = num_thread_batch_size\n",
       "transform_config.model_config.batch_size = num_thread_batch_size\n",
       "pprint(transform_config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above.\n",
       "\n",
       "Note sometimes the LLM doesn't return a JSON output, then uniflow will handle the failure and auto retry generating a new output."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "p = MultiFlowsPipeline(PipelineConfig(\n",
       "    extract_config=extract_config,\n",
       "    transform_config=transform_config,\n",
       "))\n",
       "output = p.run(data)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Process the output"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Let's take a look of the generation output. We need to do a little postprocessing on the raw output."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Extracting context, question, and answer into a DataFrame\n",
       "contexts = []\n",
       "questions = []\n",
       "answers = []\n",
       "\n",
       "for item in output:\n",
       "    for i in item.get('output', []):\n",
       "        for response in i.get('response', []):\n",
       "            if any(key not in response for key in ['context', 'question', 'answer']):\n",
       "                print(\"[WARNING] Missing context, question or answer in response, skipping:\\n\", response)\n",
       "                continue\n",
       "            if \"Claude E. Shannon\" in response['context']:\n",
       "                print(\"[WARNING] Used example context, skipping:\\n\", response[\"context\"])\n",
       "                continue\n",
       "            if len(response['context']) < 50:\n",
       "                continue\n",
       "            contexts.append(response['context'])\n",
       "            questions.append(response['question'])\n",
       "            answers.append(response['answer'])\n",
       "\n",
       "# Set display options\n",
       "pd.set_option('display.max_colwidth', None)  # or use a specific width like 50\n",
       "pd.set_option('display.width', 1000)\n",
       "\n",
       "df = pd.DataFrame({\n",
       "    'Context': contexts,\n",
       "    'Question': questions,\n",
       "    'Answer': answers\n",
       "})\n",
       "\n",
       "df.head(100)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Finally, we can save the `uniflow` output to a `.csv` file."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a570e6",
      "metadata": {},
      "outputs": [],
      "source": [
       "output_df = df[['Question', 'Answer']]\n",
       "\n",
       "output_dir = 'data/output'\n",
       "\n",
       "uniflow_output_path = f\"{output_dir}/Nike_10k_QApairs.csv\"\n",
       "\n",
       "if not os.path.exists(output_dir):\n",
       "    os.makedirs(output_dir)\n",
       "\n",
       "output_df.to_csv(uniflow_output_path, index=False)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Release GPU Memory\n",
       "We'll need to use our GPU for future steps, so let's release the memory."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "if torch.cuda.is_available():\n",
       "    torch.cuda.empty_cache()\n",
       "    print(\"GPU memory has been released.\")\n",
       "else:\n",
       "    print(\"No GPU devices found.\")\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Running `pykoi` `SupervisedFineTuning` on the QA pairs"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Install helper packages\n",
       "If you already have these installed, feel free to skip this step."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "!{sys.executable} -m pip install peft"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Import Dependency"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from pykoi.rlhf import RLHFConfig\n",
       "from pykoi.rlhf import SupervisedFinetuning\n",
       "from peft import LoraConfig, TaskType"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Set the parameters"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "base_model_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
       "dataset_name = uniflow_output_path\n",
       "peft_model_path = \"./models/rlhf_step1_sft\"\n",
       "dataset_type = \"local_csv\"\n",
       "learning_rate = 1e-3\n",
       "weight_decay = 0.0\n",
       "max_steps = 1600\n",
       "per_device_train_batch_size = 1\n",
       "per_device_eval_batch_size = 4\n",
       "log_freq = 20\n",
       "eval_freq = 2000\n",
       "save_freq = 200\n",
       "train_test_split_ratio = 0.0001\n",
       "dataset_subset_sft_train = 999999999\n",
       "size_valid_set = 0\n",
       "\n",
       "r = 8\n",
       "lora_alpha = 16\n",
       "lora_dropout = 0.05\n",
       "bias = \"none\"\n",
       "task_type = TaskType.CAUSAL_LM"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "lora_config = LoraConfig(\n",
       "    r=r,\n",
       "    lora_alpha=lora_alpha,\n",
       "    lora_dropout=lora_dropout,\n",
       "    bias=bias,\n",
       "    task_type=task_type,\n",
       "    )\n",
       "\n",
       "\n",
       "# run supervised finetuning\n",
       "config = RLHFConfig(\n",
       "    base_model_path=base_model_path,\n",
       "    dataset_type=dataset_type,\n",
       "    dataset_name=dataset_name,\n",
       "    learning_rate=learning_rate,\n",
       "    weight_decay=weight_decay,\n",
       "    max_steps=max_steps,\n",
       "    per_device_train_batch_size=per_device_train_batch_size,\n",
       "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
       "    log_freq=log_freq,\n",
       "    eval_freq=eval_freq,\n",
       "    save_freq=save_freq,\n",
       "    train_test_split_ratio=train_test_split_ratio,\n",
       "    dataset_subset_sft_train=dataset_subset_sft_train,\n",
       "    size_valid_set=size_valid_set,\n",
       "    lora_config_rl=lora_config\n",
       "    )"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Run the SupervisedFineTuning"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "rlhf_step1_sft = SupervisedFinetuning(config)\n",
       "rlhf_step1_sft.train_and_save(peft_model_path)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Release GPU Memory\n",
       "We'll need to use our GPU for future steps, so let's release the memory."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "if torch.cuda.is_available():\n",
       "    torch.cuda.empty_cache()\n",
       "    print(\"GPU memory has been released.\")\n",
       "else:\n",
       "    print(\"No GPU devices found.\")\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Running a `pykoi` `Chatbot` on the fine-tuned model\n",
       "\n",
       "### Import pykoi components"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from pykoi.application import Application\n",
       "from pykoi.chat import ModelFactory\n",
       "from pykoi.chat import QuestionAnswerDatabase\n",
       "from pykoi.component import Chatbot, Dashboard"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Create the Model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "model = ModelFactory.create_model(\n",
       "    model_source=\"peft_huggingface\",\n",
       "    base_model_path=\"meta-llama/Llama-2-7b-chat-hf\",\n",
       "    lora_model_path=\"/home/ubuntu/pykoi/models/rlhf_step1_sft\",\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Create the Chatbot with the model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "database = QuestionAnswerDatabase(debug=True)\n",
       "chatbot = Chatbot(model=model, feedback=\"vote\")\n",
       "dashboard = Dashboard(database=database)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Run the Chatbot app!"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Add `nest_asyncio` \n",
       "Add `nest_asyncio` to avoid error such as `asyncio.run() cannot be called from a running event loop`. Since we're running another interface inside a Jupyter notebook where an asyncio event loop is already running, we'll encounter the error. (since The uvicorn.run() function uses asyncio.run(), which isn't compatible with a running event loop.)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# !pip install -q nest_asyncio\n",
       "import nest_asyncio\n",
       "nest_asyncio.apply()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "app = Application(debug=False, share=False)\n",
       "app.add_component(chatbot)\n",
       "app.add_component(dashboard)\n",
       "app.run()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## End of the notebook\n",
       "\n",
       "Check more use cases in the [example folder](../../examples/)!\n",
       "\n",
       "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
       "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
       "</a>"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "self-instruct-ft",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
